{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "\n",
    "1. From BigQuery's `bigquery-public-data` project, `austin_bikeshare` dataset import `bikeshare_trips` table into a Spark DataFrame. Call it `bikeshare_trips`.\n",
    "2. Print the first few rows of `bikeshare_trips`.\n",
    "3. Print the schema for `bikeshare_trips`. You will notice that all the columns were imported as string. Do the following conversions: a) convert `duration_minutes` column to \"long\" and b) convert `start_time` column to \"timestamp\". This might be a bit tricky, we have done a similar example in our EDA notebook. You might need to use [SimpleDateFormats](https://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html) in order to define the format of the timestamp.\n",
    "4. Save the new DataFrame with the same name (replace the old one).\n",
    "5. New `bikeshare_trips`: Print the first few rows. Print the schema.\n",
    "6. Write this DataFrame to a Parquet file on your Google Cloud bucket.\n",
    "7. Load this file back into Spark and call this DataFrame `bikeshare_trips_new`.\n",
    "8. `bikeshare_trips_new`: Print the first few rows. Print the schema.\n",
    "9. Done!\n",
    "\n",
    "Note: With the sample BigQuery import code you can only import the table once. If you try to run the cell twice it will give you an error (it won't overwrite the existing file on Google Storage). The workaround is to remove the downloaded file from Google Storage and run the notebook again (pay attention where it is being stored), in case you had to rerun the BigQuery import command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following cell to get the bucket name attached to your cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is843\n",
      "is843-demo\n"
     ]
    }
   ],
   "source": [
    "bucket = spark._jsc.hadoopConfiguration().get(\"fs.gs.system.bucket\")\n",
    "project = spark._jsc.hadoopConfiguration().get(\"fs.gs.project.id\")\n",
    "\n",
    "print(bucket)\n",
    "print(project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here (use as many cells as needed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}