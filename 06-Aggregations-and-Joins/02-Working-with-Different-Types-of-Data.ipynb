{"nbformat_minor": 2, "cells": [{"source": "# Working with Different Types of Data\n\nLet's start by first loading some data.\n\nVariable `data` shows where data is located. Modify it as needed", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "data = \"gs://is843/notebooks/jupyter/data/\"", "outputs": [], "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "df = spark.read.format(\"csv\")\\\n  .option(\"header\", \"true\")\\\n  .option(\"inferSchema\", \"true\")\\\n  .load(data + \"retail-data/by-day/2010-12-01.csv\")\n\ndf.createOrReplaceTempView(\"dfTable\")\n\ndf.printSchema()\ndf.show(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: timestamp (nullable = true)\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: double (nullable = true)\n |-- Country: string (nullable = true)\n\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 5 rows\n\n"}], "metadata": {}}, {"source": "## Converting to Spark Types\n\n`lit()` function converts a type in Python to its correspnding Spark representation. Here\u2019s how we can convert a couple of different kinds of Python values to their respective Spark types:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": "from pyspark.sql.functions import lit\n\ndf.select(lit(5), lit(\"five\"), lit(5.0))", "outputs": [{"execution_count": 3, "output_type": "execute_result", "data": {"text/plain": "DataFrame[5: int, five: string, 5.0: double]"}, "metadata": {}}], "metadata": {}}, {"source": "There is no function needed for SQL:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 4, "cell_type": "code", "source": "spark.sql(\"\"\"\nSELECT 5, \"five\", 5.0\n\"\"\")", "outputs": [{"execution_count": 4, "output_type": "execute_result", "data": {"text/plain": "DataFrame[5: int, five: string, 5.0: decimal(2,1)]"}, "metadata": {}}], "metadata": {}}, {"source": "## Working with Booleans\n\nBooleans are essential when it comes to data analysis because they are the foundation for all filtering. Boolean statements consist of four elements: *and*, *or*, *true*, and *false*. We use these simple structures to build logical statements that evaluate to either *true* or *false*. These statements are often used as conditional requirements for when a row of data must either pass the test (evaluate to true) or else it will be filtered out.\n\nLet\u2019s use our retail dataset to explore working with Booleans. We can specify equality as well as less-than or greater-than:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": "from pyspark.sql.functions import col\n\ndf.where(col(\"InvoiceNo\") != 536365)\\\n  .select(\"InvoiceNo\", \"Description\")\\\n  .show(5, False)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---------+-----------------------------+\n|InvoiceNo|Description                  |\n+---------+-----------------------------+\n|536366   |HAND WARMER UNION JACK       |\n|536366   |HAND WARMER RED POLKA DOT    |\n|536367   |ASSORTED COLOUR BIRD ORNAMENT|\n|536367   |POPPY'S PLAYHOUSE BEDROOM    |\n|536367   |POPPY'S PLAYHOUSE KITCHEN    |\n+---------+-----------------------------+\nonly showing top 5 rows\n\n"}], "metadata": {}}, {"execution_count": 6, "cell_type": "code", "source": "df.where(\"InvoiceNo <> 536365\").select(\"InvoiceNo\", \"Description\").show(5, False)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---------+-----------------------------+\n|InvoiceNo|Description                  |\n+---------+-----------------------------+\n|536366   |HAND WARMER UNION JACK       |\n|536366   |HAND WARMER RED POLKA DOT    |\n|536367   |ASSORTED COLOUR BIRD ORNAMENT|\n|536367   |POPPY'S PLAYHOUSE BEDROOM    |\n|536367   |POPPY'S PLAYHOUSE KITCHEN    |\n+---------+-----------------------------+\nonly showing top 5 rows\n\n"}], "metadata": {}}, {"source": "Although you can specify your statements explicitly by using and if you like, they\u2019re often easier to understand and to read if you specify them serially. or statements need to be specified in the same statement:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 7, "cell_type": "code", "source": "from pyspark.sql.functions import instr\n\npriceFilter = col(\"UnitPrice\") > 600\ndescripFilter = instr(df.Description, \"POSTAGE\") >= 1  # instr(): Locate the position of the first occurrence of substr column in the given string.\n\ndf.where(df.StockCode.isin(\"DOT\")).where(priceFilter | descripFilter).show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|\n|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n\n"}], "metadata": {}}, {"source": "Equivalent SQL:\n\n```sql\nSELECT * FROM dfTable \nWHERE StockCode in (\"DOT\") AND \n    (UnitPrice > 600 OR instr(Description, \"POSTAGE\") >= 1)\n```", "cell_type": "markdown", "metadata": {}}, {"execution_count": 8, "cell_type": "code", "source": "from pyspark.sql.functions import instr\n\nDOTCodeFilter = col(\"StockCode\") == \"DOT\"\npriceFilter = col(\"UnitPrice\") > 600\ndescripFilter = instr(col(\"Description\"), \"POSTAGE\") >= 1\n\ndf.withColumn(\"isExpensive\", DOTCodeFilter & (priceFilter | descripFilter))\\\n  .where(\"isExpensive\")\\\n  .select(\"unitPrice\", \"isExpensive\").show(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---------+-----------+\n|unitPrice|isExpensive|\n+---------+-----------+\n|   569.77|       true|\n|   607.49|       true|\n+---------+-----------+\n\n"}], "metadata": {}}, {"execution_count": 9, "cell_type": "code", "source": "spark.sql(\"\"\"\nSELECT \n  UnitPrice, \n  (StockCode = 'DOT' AND (UnitPrice > 600 OR instr(Description, \"POSTAGE\") >= 1)) as isExpensive\nFROM dfTable\nWHERE (StockCode = 'DOT' AND (UnitPrice > 600 OR instr(Description, \"POSTAGE\") >= 1))\n\"\"\").show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---------+-----------+\n|UnitPrice|isExpensive|\n+---------+-----------+\n|   569.77|       true|\n|   607.49|       true|\n+---------+-----------+\n\n"}], "metadata": {}}, {"source": "Notice how we did not need to specify our filter as an expression and how we could use a column name without any extra work.\n\nIf you\u2019re coming from a SQL background, all of these statements should seem quite familiar. Indeed, all of them can be expressed as a where clause. In fact, it\u2019s often easier to just express filters as SQL statements than using the programmatic DataFrame interface and Spark SQL allows us to do this without paying any performance penalty. For example, the following statement uses SQL commands within `expr()`:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 10, "cell_type": "code", "source": "from pyspark.sql.functions import expr\n\ndf.withColumn(\"isExpensive\", expr(\"NOT UnitPrice <= 250\"))\\\n  .where(\"isExpensive\")\\\n  .select(\"Description\", \"UnitPrice\").show(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------+---------+\n|   Description|UnitPrice|\n+--------------+---------+\n|DOTCOM POSTAGE|   569.77|\n|DOTCOM POSTAGE|   607.49|\n+--------------+---------+\n\n"}], "metadata": {}}, {"source": "**WARNING**\n\nIf there is a null in your data, you\u2019ll need to treat things a bit differently. Here\u2019s how you can ensure that you perform a null-safe equivalence test:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 11, "cell_type": "code", "source": "df.where(col(\"Description\").eqNullSafe(\"hello\")).show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n+---------+---------+-----------+--------+-----------+---------+----------+-------+\n+---------+---------+-----------+--------+-----------+---------+----------+-------+\n\n"}], "metadata": {}}, {"source": "## Working with Numbers\n\nWhen working with big data, the second most common task you will do after filtering things is counting things. For the most part, we simply need to express our computation, and that should be valid assuming that we\u2019re working with numerical data types.\n\nTo fabricate a contrived example, let\u2019s imagine that we found out that we mis-recorded the quantity in our retail dataset and the true quantity is equal to $(Current\\_Quantity * Unit\\_Price)^2 + 5$. This will introduce our first numerical function as well as the `pow()` function that raises a column to the expressed power:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 12, "cell_type": "code", "source": "from pyspark.sql.functions import expr, pow\n\nfabricatedQuantity = pow(col(\"Quantity\") * col(\"UnitPrice\"), 2) + 5\n\ndf.select(expr(\"CustomerId\"), fabricatedQuantity.alias(\"realQuantity\")).show(2)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------+------------------+\n|CustomerId|      realQuantity|\n+----------+------------------+\n|   17850.0|239.08999999999997|\n|   17850.0|          418.7156|\n+----------+------------------+\nonly showing top 2 rows\n\n"}], "metadata": {}}, {"source": "Notice that we were able to multiply our columns together because they were both numerical. Naturally we can add and subtract as necessary, as well. In fact, we can do all of this as a SQL expression, as well:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 13, "cell_type": "code", "source": "df.selectExpr(\n  \"CustomerId\",\n  \"(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\").show(2)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------+------------------+\n|CustomerId|      realQuantity|\n+----------+------------------+\n|   17850.0|239.08999999999997|\n|   17850.0|          418.7156|\n+----------+------------------+\nonly showing top 2 rows\n\n"}], "metadata": {}}, {"source": "In SQL:\n```sql\nSELECT customerId, (POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\nFROM dfTable\n```\n\nAnother common numerical task is rounding. If you\u2019d like to just round to a whole number, oftentimes you can cast the value to an integer and that will work just fine. However, Spark also has more detailed functions for performing this explicitly and to a certain level of precision. In the following example, we round to one decimal place:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 14, "cell_type": "code", "source": "from pyspark.sql.functions import lit, round, bround\n\ndf.select(round(lit(\"2.5\")), bround(lit(\"2.5\"))).show(1)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------------+--------------+\n|round(2.5, 0)|bround(2.5, 0)|\n+-------------+--------------+\n|          3.0|           2.0|\n+-------------+--------------+\nonly showing top 1 row\n\n"}], "metadata": {}}, {"source": "In SQL\n```sql\nSELECT round(2.5), bround(2.5)\n```\n\nAnother numerical task is to compute the correlation of two columns. For example, we can see the Pearson correlation coefficient for two columns to see if cheaper things are typically bought in greater quantities. We can do this through a function as well as through the DataFrame statistic methods:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 15, "cell_type": "code", "source": "df.stat.corr(\"Quantity\", \"UnitPrice\")", "outputs": [{"execution_count": 15, "output_type": "execute_result", "data": {"text/plain": "-0.04112314436835551"}, "metadata": {}}], "metadata": {}}, {"execution_count": 16, "cell_type": "code", "source": "from pyspark.sql.functions import corr\n\ndf.select(corr(\"Quantity\", \"UnitPrice\")).show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------------------------+\n|corr(Quantity, UnitPrice)|\n+-------------------------+\n|     -0.04112314436835551|\n+-------------------------+\n\n"}], "metadata": {}}, {"source": "In SQL\n```sql\nSELECT corr(Quantity, UnitPrice) FROM dfTable\n```\n\nAnother common task is to compute summary statistics for a column or set of columns. We can use the `describe` method to achieve exactly this. This will take all numerical and string columns and calculate the count, mean, standard deviation, min, and max:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 17, "cell_type": "code", "source": "df.describe(['Quantity', 'UnitPrice', 'Country']).show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------+------------------+------------------+--------------+\n|summary|          Quantity|         UnitPrice|       Country|\n+-------+------------------+------------------+--------------+\n|  count|              3108|              3108|          3108|\n|   mean| 8.627413127413128| 4.151946589446603|          null|\n| stddev|26.371821677029203|15.638659854603892|          null|\n|    min|               -24|               0.0|     Australia|\n|    max|               600|            607.49|United Kingdom|\n+-------+------------------+------------------+--------------+\n\n"}], "metadata": {}}, {"source": "If you need these exact numbers, you can also perform this as an aggregation yourself by importing the functions and applying them to the columns that you need:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 18, "cell_type": "code", "source": "from pyspark.sql.functions import count, mean, stddev_pop, min, max", "outputs": [], "metadata": {}}, {"source": "There are a number of statistical functions available in the `StatFunctions` Package (accessible using stat as we see in the code block below). These are DataFrame methods that you can use to calculate a variety of different things. For instance, you can calculate either exact or approximate quantiles of your data using the `approxQuantile` method:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 19, "cell_type": "code", "source": "colName = \"UnitPrice\"\nquantileProbs = [0.5]\nrelError = 0.05\n\ndf.stat.approxQuantile(\"UnitPrice\", quantileProbs, relError)", "outputs": [{"execution_count": 19, "output_type": "execute_result", "data": {"text/plain": "[2.51]"}, "metadata": {}}], "metadata": {}}, {"source": "Finding frequent items for columns:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 20, "cell_type": "code", "source": "df.stat.freqItems([\"StockCode\", \"Quantity\"]).show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+--------------------+\n| StockCode_freqItems|  Quantity_freqItems|\n+--------------------+--------------------+\n|[90214E, 20728, 2...|[200, 128, 23, 32...|\n+--------------------+--------------------+\n\n"}], "metadata": {}}, {"source": "As a last note, we can also add a unique ID to each row by using the function `monotonically_increasing_id`. This function generates a unique value for each row, starting with 0:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 21, "cell_type": "code", "source": "from pyspark.sql.functions import monotonically_increasing_id\n\ndf.select(monotonically_increasing_id(), \"StockCode\", \"Quantity\", \"UnitPrice\").show(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----------------------------+---------+--------+---------+\n|monotonically_increasing_id()|StockCode|Quantity|UnitPrice|\n+-----------------------------+---------+--------+---------+\n|                            0|   85123A|       6|     2.55|\n|                            1|    71053|       6|     3.39|\n|                            2|   84406B|       8|     2.75|\n|                            3|   84029G|       6|     3.39|\n|                            4|   84029E|       6|     3.39|\n+-----------------------------+---------+--------+---------+\nonly showing top 5 rows\n\n"}], "metadata": {}}, {"source": "## Working with Strings\n\nThe `initcap` function will capitalize every word in a given string, with the first letter of each word in uppercase, all other letters in lowercase:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 22, "cell_type": "code", "source": "from pyspark.sql.functions import initcap\n\ndf.select(initcap(col(\"Description\"))).show(5, False)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----------------------------------+\n|initcap(Description)               |\n+-----------------------------------+\n|White Hanging Heart T-light Holder |\n|White Metal Lantern                |\n|Cream Cupid Hearts Coat Hanger     |\n|Knitted Union Flag Hot Water Bottle|\n|Red Woolly Hottie White Heart.     |\n+-----------------------------------+\nonly showing top 5 rows\n\n"}], "metadata": {}}, {"source": "You can cast strings in uppercase and lowercase, as well:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 23, "cell_type": "code", "source": "from pyspark.sql.functions import lower, upper\n\ndf.select(col(\"Description\"),\n    lower(col(\"Description\")),\n    upper(col(\"Description\"))).show(2, False)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------------------------------+----------------------------------+----------------------------------+\n|Description                       |lower(Description)                |upper(Description)                |\n+----------------------------------+----------------------------------+----------------------------------+\n|WHITE HANGING HEART T-LIGHT HOLDER|white hanging heart t-light holder|WHITE HANGING HEART T-LIGHT HOLDER|\n|WHITE METAL LANTERN               |white metal lantern               |WHITE METAL LANTERN               |\n+----------------------------------+----------------------------------+----------------------------------+\nonly showing top 2 rows\n\n"}], "metadata": {}}, {"source": "In SQL\n```sql\nSELECT Description, lower(Description), upper(Description) FROM dfTable\n```\n\nAnother trivial task is adding or removing spaces around a string. You can do this by using `lpad`, `ltrim`, `rpad` and `rtrim`, `trim`:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 24, "cell_type": "code", "source": "from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim\n\ndf.select(\n    ltrim(lit(\"    HELLO    \")).alias(\"ltrim\"),\n    rtrim(lit(\"    HELLO    \")).alias(\"rtrim\"),\n    trim(lit(\"    HELLO    \")).alias(\"trim\"),\n    lpad(lit(\"HELLO\"), 3, \" \").alias(\"lp\"),\n    rpad(lit(\"HELLO\"), 10, \" \").alias(\"rp\")).show(1)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---------+---------+-----+---+----------+\n|    ltrim|    rtrim| trim| lp|        rp|\n+---------+---------+-----+---+----------+\n|HELLO    |    HELLO|HELLO|HEL|HELLO     |\n+---------+---------+-----+---+----------+\nonly showing top 1 row\n\n"}], "metadata": {}}, {"source": "In SQL\n```sql\nSELECT\n  ltrim('    HELLLOOOO  '),\n  rtrim('    HELLLOOOO  '),\n  trim('    HELLLOOOO  '),\n  lpad('HELLOOOO  ', 3, ' '),\n  rpad('HELLOOOO  ', 10, ' ')\nFROM dfTable\n```\n\nNote that if lpad or rpad takes a number less than the length of the string, it will always remove values from the right side of the string.", "cell_type": "markdown", "metadata": {}}, {"source": "## Regular Expressions\n\nProbably one of the most frequently performed tasks is searching for the existence of one string in another or replacing all mentions of a string with another value. This is often done with a tool called *regular expressions* that exists in many programming languages.\n\nSpark takes advantage of the complete power of Java regular expressions. There are two key functions in Spark that you\u2019ll need in order to perform regular expression tasks: `regexp_extract` and `regexp_replace`. These functions extract values and replace values, respectively.\n\nLet\u2019s explore how to use the `regexp_replace` function to replace substitute color names in our description column:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 25, "cell_type": "code", "source": "from pyspark.sql.functions import regexp_replace\n\nregex_string = \"BLACK|WHITE|RED|GREEN|BLUE\"\n\ndf.select(\n  regexp_replace(col(\"Description\"), regex_string, \"COLOR\").alias(\"color_clean\"),\n  col(\"Description\")).show(2)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+--------------------+\n|         color_clean|         Description|\n+--------------------+--------------------+\n|COLOR HANGING HEA...|WHITE HANGING HEA...|\n| COLOR METAL LANTERN| WHITE METAL LANTERN|\n+--------------------+--------------------+\nonly showing top 2 rows\n\n"}], "metadata": {}}, {"source": "In SQL\n```sql\nSELECT\n  regexp_replace(Description, 'BLACK|WHITE|RED|GREEN|BLUE', 'COLOR') as\n  color_clean, Description\nFROM dfTable\n```", "cell_type": "markdown", "metadata": {}}, {"source": "Another task might be to replace given characters with other characters. Building this as a regular expression could be tedious, so Spark also provides the `translate` function to replace these values. This is done at the character level and will replace all instances of a character with the indexed character in the replacement string:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 26, "cell_type": "code", "source": "from pyspark.sql.functions import translate\n\ndf.select(translate(col(\"Description\"), \"LEET\", \"1337\"),col(\"Description\"))\\\n  .show(2, False)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------------------------------+----------------------------------+\n|translate(Description, LEET, 1337)|Description                       |\n+----------------------------------+----------------------------------+\n|WHI73 HANGING H3AR7 7-1IGH7 HO1D3R|WHITE HANGING HEART T-LIGHT HOLDER|\n|WHI73 M37A1 1AN73RN               |WHITE METAL LANTERN               |\n+----------------------------------+----------------------------------+\nonly showing top 2 rows\n\n"}], "metadata": {}}, {"source": "In SQL\n```sql\nSELECT translate(Description, 'LEET', '1337'), Description FROM dfTable\n```\n\nWe can also perform something similar, like pulling out the first mentioned color:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 27, "cell_type": "code", "source": "from pyspark.sql.functions import regexp_extract\n\nextract_str = \"(BLACK|WHITE|RED|GREEN|BLUE)\"\n\ndf.select(\n     regexp_extract(col(\"Description\"), extract_str, 1).alias(\"color_clean\"),\n     col(\"Description\")).show(5, False)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----------+-----------------------------------+\n|color_clean|Description                        |\n+-----------+-----------------------------------+\n|WHITE      |WHITE HANGING HEART T-LIGHT HOLDER |\n|WHITE      |WHITE METAL LANTERN                |\n|           |CREAM CUPID HEARTS COAT HANGER     |\n|           |KNITTED UNION FLAG HOT WATER BOTTLE|\n|RED        |RED WOOLLY HOTTIE WHITE HEART.     |\n+-----------+-----------------------------------+\nonly showing top 5 rows\n\n"}], "metadata": {}}, {"source": "In SQL\n```sql\nSELECT regexp_extract(Description, '(BLACK|WHITE|RED|GREEN|BLUE)', 1),\n  Description\nFROM dfTable\n```\n\nSometimes, rather than extracting values, we simply want to check for their existence. We can do this with the `instr` method on each column. This will return a *Boolean* declaring whether the value you specify is in the column\u2019s string:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 28, "cell_type": "code", "source": "from pyspark.sql.functions import instr\n\ncontainsBlack = instr(col(\"Description\"), \"BLACK\") >= 1\ncontainsWhite = instr(col(\"Description\"), \"WHITE\") >= 1\n\ndf.withColumn(\"hasSimpleColor\", containsBlack | containsWhite)\\\n  .where(\"hasSimpleColor\")\\\n  .select(\"Description\").show(3, False)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------------------------------+\n|Description                       |\n+----------------------------------+\n|WHITE HANGING HEART T-LIGHT HOLDER|\n|WHITE METAL LANTERN               |\n|RED WOOLLY HOTTIE WHITE HEART.    |\n+----------------------------------+\nonly showing top 3 rows\n\n"}], "metadata": {}}, {"source": "In SQL\n```sql\nSELECT Description FROM dfTable\nWHERE instr(Description, 'BLACK') >= 1 OR instr(Description, 'WHITE') >= 1\n```\n\nThis is trivial with just two values, but it becomes more complicated when there are values.\n\nLet\u2019s work through this in a more rigorous way and take advantage of Spark\u2019s ability to accept a dynamic number of arguments. When we convert a list of values into a set of arguments and pass them into a function, we use a language feature called varargs. Using this feature, we can effectively unravel an array of arbitrary length and pass it as arguments to a function. \n\nWe can also do this quite easily in Python. In this case, we\u2019re going to use a different function, `locate`, that returns the integer location (1 based location). We then convert that to a Boolean before using it as the same basic feature:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 29, "cell_type": "code", "source": "from pyspark.sql.functions import expr, locate\nsimpleColors = [\"black\", \"white\", \"red\", \"green\", \"blue\"]\ndef color_locator(column, color_string):\n    return locate(color_string.upper(), column)\\\n        .cast(\"boolean\")\\\n        .alias(\"is_\" + color_string)\nselectedColumns = [color_locator(df.Description, c) for c in simpleColors]\nselectedColumns.append(expr(\"*\")) # has to be a Column type\n\ndf.select(*selectedColumns).where(expr(\"is_white OR is_red\"))\\\n  .select(\"Description\").show(3, False)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------------------------------+\n|Description                       |\n+----------------------------------+\n|WHITE HANGING HEART T-LIGHT HOLDER|\n|WHITE METAL LANTERN               |\n|RED WOOLLY HOTTIE WHITE HEART.    |\n+----------------------------------+\nonly showing top 3 rows\n\n"}], "metadata": {}}, {"execution_count": 30, "cell_type": "code", "source": "df.select(*selectedColumns)", "outputs": [{"execution_count": 30, "output_type": "execute_result", "data": {"text/plain": "DataFrame[is_black: boolean, is_white: boolean, is_red: boolean, is_green: boolean, is_blue: boolean, InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"}, "metadata": {}}], "metadata": {}}, {"source": "## Working with Dates and Timestamps\n\nLet\u2019s begin with the basics and get the current date and the current timestamps:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 31, "cell_type": "code", "source": "from pyspark.sql.functions import current_date, current_timestamp\n\ndateDF = spark.range(10)\\\n  .withColumn(\"today\", current_date())\\\n  .withColumn(\"now\", current_timestamp())\n\ndateDF.createOrReplaceTempView(\"dateTable\")", "outputs": [], "metadata": {}}, {"execution_count": 32, "cell_type": "code", "source": "dateDF.printSchema()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- id: long (nullable = false)\n |-- today: date (nullable = false)\n |-- now: timestamp (nullable = false)\n\n"}], "metadata": {}}, {"execution_count": 33, "cell_type": "code", "source": "dateDF.show(5, False)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---+----------+-----------------------+\n|id |today     |now                    |\n+---+----------+-----------------------+\n|0  |2020-03-17|2020-03-17 18:31:34.946|\n|1  |2020-03-17|2020-03-17 18:31:34.946|\n|2  |2020-03-17|2020-03-17 18:31:34.946|\n|3  |2020-03-17|2020-03-17 18:31:34.946|\n|4  |2020-03-17|2020-03-17 18:31:34.946|\n+---+----------+-----------------------+\nonly showing top 5 rows\n\n"}], "metadata": {}}, {"source": "Notice that `current_timestamp()` records the current time of when the action which is `show()` takes place, not when the transformation, dateDF creation, was placed.\n\nAlso, notice that the date/time is being recorded as GMT. One should always have in mind which timezone is being used. You can set a session local timezone if necessary by setting spark.conf.sessionLocalTimeZone in the SQL configurations. \n\nNow that we have a simple DataFrame to work with, let\u2019s add and subtract five days from today. These functions take a column and then the number of days to either add or subtract as the arguments:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 34, "cell_type": "code", "source": "from pyspark.sql.functions import date_add, date_sub\n\ndateDF.select(date_sub(col(\"today\"), 5), date_add(col(\"today\"), 5)).show(1)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+------------------+------------------+\n|date_sub(today, 5)|date_add(today, 5)|\n+------------------+------------------+\n|        2020-03-12|        2020-03-22|\n+------------------+------------------+\nonly showing top 1 row\n\n"}], "metadata": {}}, {"source": "In SQL\n```sql\nSELECT date_sub(today, 5), date_add(today, 5) FROM dateTable\n```\n\nAnother common task is to take a look at the difference between two dates. We can do this with the `datediff` function that will return the number of days in between two dates. Most often we just care about the days, and because the number of days varies from month to month, there also exists a function, `months_between`, that gives you the number of months between two dates:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 35, "cell_type": "code", "source": "from pyspark.sql.functions import datediff, months_between, to_date\n\ndateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\\\n  .select(\"week_ago\", \"today\", datediff(col(\"week_ago\"), col(\"today\"))).show(1)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------+----------+-------------------------+\n|  week_ago|     today|datediff(week_ago, today)|\n+----------+----------+-------------------------+\n|2020-03-10|2020-03-17|                       -7|\n+----------+----------+-------------------------+\nonly showing top 1 row\n\n"}], "metadata": {}}, {"execution_count": 36, "cell_type": "code", "source": "dateDF.select(\n    to_date(lit(\"2016-01-01\")).alias(\"start\"),\n    to_date(lit(\"2017-05-22\")).alias(\"end\"))\\\n  .select(\"start\", \"end\", months_between(col(\"start\"), col(\"end\"))).show(1)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------+----------+--------------------------+\n|     start|       end|months_between(start, end)|\n+----------+----------+--------------------------+\n|2016-01-01|2017-05-22|              -16.67741935|\n+----------+----------+--------------------------+\nonly showing top 1 row\n\n"}], "metadata": {}}, {"source": "In SQL\n```sql\nSELECT to_date('2016-01-01'), months_between('2016-01-01', '2017-01-01'),\ndatediff('2016-01-01', '2017-01-01')\nFROM dateTable\n```\n\nNotice that we introduced a new function: the `to_date` function. The `to_date` function allows you to convert a string to a date, optionally with a specified format. We specify our format in the [Java SimpleDateFormat](https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html) which will be important to reference if you use this function:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 37, "cell_type": "code", "source": "from pyspark.sql.functions import to_date, lit\n\nspark.range(5).withColumn(\"date\", lit(\"2017-01-01\"))\\\n  .select(to_date(col(\"date\"))).show(1)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---------------+\n|to_date(`date`)|\n+---------------+\n|     2017-01-01|\n+---------------+\nonly showing top 1 row\n\n"}], "metadata": {}}, {"source": "Spark will not throw an error if it cannot parse the date; rather, it will just return null. This can be a bit tricky in larger pipelines because you might be expecting your data in one format and getting it in another. To illustrate, let\u2019s take a look at the date format that has switched from year-month-day to year-day-month. Spark will fail to parse this date and silently return null instead:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 38, "cell_type": "code", "source": "dateDF.select(to_date(lit(\"2016-20-12\")),to_date(lit(\"2017-12-11\"))).show(1)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---------------------+---------------------+\n|to_date('2016-20-12')|to_date('2017-12-11')|\n+---------------------+---------------------+\n|                 null|           2017-12-11|\n+---------------------+---------------------+\nonly showing top 1 row\n\n"}], "metadata": {}}, {"source": "We find this to be an especially tricky situation for bugs because some dates might match the correct format, whereas others do not. In the previous example, notice how the second date appears as Decembers 11th instead of the correct day, November 12th. Spark doesn\u2019t throw an error because it cannot know whether the days are mixed up or that specific row is incorrect.\n\nLet\u2019s fix this pipeline, step by step, and come up with a robust way to avoid these issues entirely. The first step is to remember that we need to specify our date format according to the Java SimpleDateFormat standard.\n\nWe will use two functions to fix this: `to_date` and `to_timestamp`. The former optionally expects a format, whereas the latter requires one:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 39, "cell_type": "code", "source": "from pyspark.sql.functions import to_date\n\ndateFormat = \"yyyy-dd-MM\"\n\ncleanDateDF = spark.range(1).select(\n    to_date(lit(\"2017-12-11\"), dateFormat).alias(\"date\"),\n    to_date(lit(\"2017-20-12\"), dateFormat).alias(\"date2\"))\n\ncleanDateDF.show()\n\ncleanDateDF.createOrReplaceTempView(\"dateTable2\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------+----------+\n|      date|     date2|\n+----------+----------+\n|2017-11-12|2017-12-20|\n+----------+----------+\n\n"}], "metadata": {}}, {"source": "In SQL\n```sql\nSELECT to_date(date, 'yyyy-dd-MM'), to_date(date2, 'yyyy-dd-MM'), to_date(date)\nFROM dateTable2\n```\n\nNow let\u2019s use an example of to_timestamp, which always requires a format to be specified:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 40, "cell_type": "code", "source": "from pyspark.sql.functions import to_timestamp\n\ncleanDateDF.select(to_timestamp(col(\"date\"), \"yyyy-dd-MM\")).show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------------------------------+\n|to_timestamp(`date`, 'yyyy-dd-MM')|\n+----------------------------------+\n|               2017-11-12 00:00:00|\n+----------------------------------+\n\n"}], "metadata": {}}, {"source": "After we have our date or timestamp in the correct format and type, comparing between them is actually quite easy. We just need to be sure to either use a date/timestamp type or specify our string according to the right format of *yyyy-MM-dd* if we\u2019re comparing a date:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 41, "cell_type": "code", "source": "cleanDateDF.where(col(\"date2\") > lit(\"2017-12-12\")).show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------+----------+\n|      date|     date2|\n+----------+----------+\n|2017-11-12|2017-12-20|\n+----------+----------+\n\n"}], "metadata": {}}, {"source": "One minor point is that we can also set this as a string, which Spark parses to a literal:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 42, "cell_type": "code", "source": "cleanDateDF.filter(col(\"date2\") > \"2017-12-12\").show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------+----------+\n|      date|     date2|\n+----------+----------+\n|2017-11-12|2017-12-20|\n+----------+----------+\n\n"}], "metadata": {}}, {"source": "It is important to point out that a good practice is to parse the values explicitly instead of relying on implicit conversions.\n\n## Working with Nulls in Data\n\nAs a best practice, you should always use nulls to represent missing or empty data in your DataFrames. Spark can optimize working with null values more than it can if you use empty strings or other values. The primary way of interacting with null values, at DataFrame scale, is to use the `.na` subpackage on a DataFrame. There are also several functions for performing operations and explicitly specifying how Spark should handle null values.\n\n**WARNING**\n\nNulls are a challenging part of all programming, and Spark is no exception. Being explicit is always better than being implicit when handling null values. When we declare a column as not having a null time, that is not actually enforced. To reiterate, when you define a schema in which all columns are declared to not have null values, Spark will not enforce that and will happily let null values into that column. The nullable signal is simply to help Spark SQL optimize for handling that column. If you have null values in columns that should not have null values, you can get an incorrect result or see strange exceptions that can be difficult to debug.\n\nThere are two things you can do with null values: you can explicitly drop nulls or you can fill them with a value (globally or on a per-column basis). Let\u2019s experiment with each of these now.\n\n### Coalesce\n\nSpark includes a function to allow you to select the first non-null value from a set of columns by using the `coalesce` function. In this case, there are no null values, so it simply returns the first column:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 43, "cell_type": "code", "source": "from pyspark.sql.functions import coalesce\n\ndf.select(coalesce(col(\"Description\"), col(\"CustomerId\"))).show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---------------------------------+\n|coalesce(Description, CustomerId)|\n+---------------------------------+\n|             WHITE HANGING HEA...|\n|              WHITE METAL LANTERN|\n|             CREAM CUPID HEART...|\n|             KNITTED UNION FLA...|\n|             RED WOOLLY HOTTIE...|\n|             SET 7 BABUSHKA NE...|\n|             GLASS STAR FROSTE...|\n|             HAND WARMER UNION...|\n|             HAND WARMER RED P...|\n|             ASSORTED COLOUR B...|\n|             POPPY'S PLAYHOUSE...|\n|             POPPY'S PLAYHOUSE...|\n|             FELTCRAFT PRINCES...|\n|             IVORY KNITTED MUG...|\n|             BOX OF 6 ASSORTED...|\n|             BOX OF VINTAGE JI...|\n|             BOX OF VINTAGE AL...|\n|             HOME BUILDING BLO...|\n|             LOVE BUILDING BLO...|\n|             RECIPE BOX WITH M...|\n+---------------------------------+\nonly showing top 20 rows\n\n"}], "metadata": {}}, {"source": "### `ifnull`, `nullIf`, `nvl`, and `nvl2`\n\nThere are several other SQL functions that you can use to achieve similar things. \n\nThe `ifnull` and `nvl` functions are synonyms: ifnull(expr1, expr2) - Returns expr2 if expr1 is null, or expr1 otherwise.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 44, "cell_type": "code", "source": "spark.sql(\"\"\"\nSELECT\n  ifnull(null, 'expr2'),\n  ifnull('expr1', 'expr2'),\n  nvl(null, 'expr2')\nFROM dfTable LIMIT 1\n\"\"\").show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---------------------+------------------------+------------------+\n|ifnull(NULL, 'expr2')|ifnull('expr1', 'expr2')|nvl(NULL, 'expr2')|\n+---------------------+------------------------+------------------+\n|                expr2|                   expr1|             expr2|\n+---------------------+------------------------+------------------+\n\n"}], "metadata": {}}, {"source": "`nullif`: nullif(expr1, expr2) - Returns null if expr1 equals to expr2, or expr1 otherwise.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 45, "cell_type": "code", "source": "spark.sql(\"\"\"\nSELECT\n  nullif('expr1', 'expr1'),\n  nullif('expr1', 'expr2')\nFROM dfTable LIMIT 1\n\"\"\").show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+------------------------+------------------------+\n|nullif('expr1', 'expr1')|nullif('expr1', 'expr2')|\n+------------------------+------------------------+\n|                    null|                   expr1|\n+------------------------+------------------------+\n\n"}], "metadata": {}}, {"source": "`nvl2`: nvl2(expr1, expr2, expr3) - Returns expr2 if expr1 is not null, or expr3 otherwise.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 46, "cell_type": "code", "source": "spark.sql(\"\"\"\nSELECT\n  nvl2('expr1', 'expr2', \"expr3\"),\n  nvl2(null, 'expr2', \"expr3\")\nFROM dfTable LIMIT 1\n\"\"\").show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------------------------------+----------------------------+\n|nvl2('expr1', 'expr2', 'expr3')|nvl2(NULL, 'expr2', 'expr3')|\n+-------------------------------+----------------------------+\n|                          expr2|                       expr3|\n+-------------------------------+----------------------------+\n\n"}], "metadata": {}}, {"source": "Naturally, we can use these in select expressions on DataFrames, as well.\n\n### `isNull`\n\n`isNull(expr)` - Returns true if expr is null, or false otherwise.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 47, "cell_type": "code", "source": "df.where(col(\"UnitPrice\").isNull()).show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n+---------+---------+-----------+--------+-----------+---------+----------+-------+\n+---------+---------+-----------+--------+-----------+---------+----------+-------+\n\n"}], "metadata": {}}, {"execution_count": 48, "cell_type": "code", "source": "df.where(col(\"CustomerID\").isNull()).show(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536414|    22139|                null|      56|2010-12-01 11:52:00|      0.0|      null|United Kingdom|\n|   536544|    21773|DECORATIVE ROSE B...|       1|2010-12-01 14:32:00|     2.51|      null|United Kingdom|\n|   536544|    21774|DECORATIVE CATS B...|       2|2010-12-01 14:32:00|     2.51|      null|United Kingdom|\n|   536544|    21786|  POLKADOT RAIN HAT |       4|2010-12-01 14:32:00|     0.85|      null|United Kingdom|\n|   536544|    21787|RAIN PONCHO RETRO...|       2|2010-12-01 14:32:00|     1.66|      null|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 5 rows\n\n"}], "metadata": {}}, {"source": "Let's get a count of how many null values exist in each column using a loop and `isNull` function:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 49, "cell_type": "code", "source": "[(c, df.where(col(c).isNull()).count()) for c in df.columns]", "outputs": [{"execution_count": 49, "output_type": "execute_result", "data": {"text/plain": "[('InvoiceNo', 0),\n ('StockCode', 0),\n ('Description', 10),\n ('Quantity', 0),\n ('InvoiceDate', 0),\n ('UnitPrice', 0),\n ('CustomerID', 1140),\n ('Country', 0)]"}, "metadata": {}}], "metadata": {}}, {"execution_count": 50, "cell_type": "code", "source": "print(\"DataFrame df consists of {} records, out of which {} records have a missing CustomerID, {} have a missing Description, and {} are missing both!\"\\\n      .format(df.count(),\n             df.where(col(\"CustomerID\").isNull()).count(),\n             df.where(col(\"Description\").isNull()).count(),\n             df.where(col(\"CustomerID\").isNull() & col(\"Description\").isNull()).count()))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "DataFrame df consists of 3108 records, out of which 1140 records have a missing CustomerID, 10 have a missing Description, and 10 are missing both!\n"}], "metadata": {}}, {"execution_count": 51, "cell_type": "code", "source": "df.where(col(\"CustomerID\").isNull() & col(\"Description\").isNull()).show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n|   536414|    22139|       null|      56|2010-12-01 11:52:00|      0.0|      null|United Kingdom|\n|   536545|    21134|       null|       1|2010-12-01 14:32:00|      0.0|      null|United Kingdom|\n|   536546|    22145|       null|       1|2010-12-01 14:33:00|      0.0|      null|United Kingdom|\n|   536547|    37509|       null|       1|2010-12-01 14:33:00|      0.0|      null|United Kingdom|\n|   536549|   85226A|       null|       1|2010-12-01 14:34:00|      0.0|      null|United Kingdom|\n|   536550|    85044|       null|       1|2010-12-01 14:34:00|      0.0|      null|United Kingdom|\n|   536552|    20950|       null|       1|2010-12-01 14:34:00|      0.0|      null|United Kingdom|\n|   536553|    37461|       null|       3|2010-12-01 14:35:00|      0.0|      null|United Kingdom|\n|   536554|    84670|       null|      23|2010-12-01 14:35:00|      0.0|      null|United Kingdom|\n|   536589|    21777|       null|     -10|2010-12-01 16:50:00|      0.0|      null|United Kingdom|\n+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n\n"}], "metadata": {}}, {"source": "### Negating conditions within a filter\nWe can use `~` within `where()` to negate the filter. For instance this can be used in conjunction with `isNull()` to filter values that are not null:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 52, "cell_type": "code", "source": "df.where(~col(\"UnitPrice\").isNull()).show(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 5 rows\n\n"}], "metadata": {}}, {"source": "### `drop`\n\nThe simplest function is `drop`, which removes rows that contain nulls. The default is to drop any row in which any value is null:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 53, "cell_type": "code", "source": "df.na.drop()", "outputs": [{"execution_count": 53, "output_type": "execute_result", "data": {"text/plain": "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"}, "metadata": {}}], "metadata": {}}, {"source": "Equivalent to \n```python \ndf.na.drop(\"any\")\n```\n\nIn SQL, we have to do this column by column:\n\n```sql\nSELECT * FROM dfTable WHERE Description IS NOT NULL\n```\n\nSpecifying \"`any`\" as an argument drops a row if any of the values are null. \n\nLet's check and see how many rows were actually dropped:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 54, "cell_type": "code", "source": "df.count() - df.na.drop().count()", "outputs": [{"execution_count": 54, "output_type": "execute_result", "data": {"text/plain": "1140"}, "metadata": {}}], "metadata": {}}, {"source": "Which is what we expected, based on the above summary.\n\nUsing \u201call\u201d drops the row only if all values are `null` or `NaN` for that row:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 55, "cell_type": "code", "source": "df.na.drop(\"all\")", "outputs": [{"execution_count": 55, "output_type": "execute_result", "data": {"text/plain": "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"}, "metadata": {}}], "metadata": {}}, {"source": "Since we don't have any rows with all columns being null this doesn't drop anything:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 56, "cell_type": "code", "source": "df.na.drop(\"all\").count()", "outputs": [{"execution_count": 56, "output_type": "execute_result", "data": {"text/plain": "3108"}, "metadata": {}}], "metadata": {}}, {"source": "We can also apply this to certain sets of columns by passing in an array of columns:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 57, "cell_type": "code", "source": "df.na.drop(\"all\", subset=[\"CustomerID\", \"Description\"])", "outputs": [{"execution_count": 57, "output_type": "execute_result", "data": {"text/plain": "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"}, "metadata": {}}], "metadata": {}}, {"source": "The following code shows that the above command will drop 10 records that have both *CustomerID* and *Description* missing.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 58, "cell_type": "code", "source": "df.count() - df.na.drop(\"all\", subset=[\"CustomerID\", \"Description\"]).count()", "outputs": [{"execution_count": 58, "output_type": "execute_result", "data": {"text/plain": "10"}, "metadata": {}}], "metadata": {}}, {"source": "### `fill`\n\nUsing the `fil`l function, you can \"fill\" one or more columns with a set of values. This can be done by specifying a map\u2014 that is a particular value and a set of columns.\n\nFor example, to fill all null values in columns of type String, you might specify a string:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 59, "cell_type": "code", "source": "df.printSchema()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: timestamp (nullable = true)\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: double (nullable = true)\n |-- Country: string (nullable = true)\n\n"}], "metadata": {}}, {"execution_count": 60, "cell_type": "code", "source": "df.where(col(\"Description\").isNull())\\\n  .na.fill(\"No Value\").show(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n|   536414|    22139|   No Value|      56|2010-12-01 11:52:00|      0.0|      null|United Kingdom|\n|   536545|    21134|   No Value|       1|2010-12-01 14:32:00|      0.0|      null|United Kingdom|\n|   536546|    22145|   No Value|       1|2010-12-01 14:33:00|      0.0|      null|United Kingdom|\n|   536547|    37509|   No Value|       1|2010-12-01 14:33:00|      0.0|      null|United Kingdom|\n|   536549|   85226A|   No Value|       1|2010-12-01 14:34:00|      0.0|      null|United Kingdom|\n+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\nonly showing top 5 rows\n\n"}], "metadata": {}}, {"source": "We could do the same for columns of type Integer by using df.na.fill(5:Integer), or for Doubles df.na.fill(5:Double). To specify columns, we just pass in an array of column names like we did in the previous example:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 61, "cell_type": "code", "source": "df.where(col(\"CustomerID\").isNull())\\\n  .na.fill(0.0).show(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536414|    22139|                null|      56|2010-12-01 11:52:00|      0.0|       0.0|United Kingdom|\n|   536544|    21773|DECORATIVE ROSE B...|       1|2010-12-01 14:32:00|     2.51|       0.0|United Kingdom|\n|   536544|    21774|DECORATIVE CATS B...|       2|2010-12-01 14:32:00|     2.51|       0.0|United Kingdom|\n|   536544|    21786|  POLKADOT RAIN HAT |       4|2010-12-01 14:32:00|     0.85|       0.0|United Kingdom|\n|   536544|    21787|RAIN PONCHO RETRO...|       2|2010-12-01 14:32:00|     1.66|       0.0|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 5 rows\n\n"}], "metadata": {}}, {"source": "Please note that in the code above the filtering is just for \"showing\" purposes and \"fill\" will get applied to all the columns with type Double.\n\nWe can also do this with a Python dictionary, where the key is the column name and the value is the value we would like to use to fill null values:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 62, "cell_type": "code", "source": "fill_cols_vals = {\"CustomerID\": 0.0, \"Description\" : \"No Value\"}\ndf2 = df.na.fill(fill_cols_vals)\n\ndf2.where(col(\"CustomerID\").isNull() | col(\"Description\").isNull()).show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n+---------+---------+-----------+--------+-----------+---------+----------+-------+\n+---------+---------+-----------+--------+-----------+---------+----------+-------+\n\n"}], "metadata": {}}, {"source": "We can see that all the null values have been replaced with a non-null that we specidied for df2. \n\n### `replace`\n\nIn addition to replacing null values like we did with `drop` and `fill`, there are more flexible options that you can use with more than just null values. Probably the most common use case is to replace all values in a certain column according to their current value. The only requirement is that this value be the same type as the original value:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 63, "cell_type": "code", "source": "df.na.replace([\"\"], [\"UNKNOWN\"], \"Description\")", "outputs": [{"execution_count": 63, "output_type": "execute_result", "data": {"text/plain": "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"}, "metadata": {}}], "metadata": {}}, {"source": "In this particular example we didn't have an empty string in Description to replace it with UNKNONW:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 64, "cell_type": "code", "source": "df.na.replace([\"\"], [\"UNKNOWN\"], \"Description\").where(\"Description = 'UNKNOWN'\").count()", "outputs": [{"execution_count": 64, "output_type": "execute_result", "data": {"text/plain": "0"}, "metadata": {}}], "metadata": {}}, {"source": "## Ordering\n\nYou can use `asc_nulls_first`, `desc_nulls_first`, `asc_nulls_last`, or `desc_nulls_last` to specify where you would like your null values to appear in an ordered DataFrame.\n\n**Note**\n\nThis is a new feature in Spark 2.4 and is not yet working properly. The PySpark code for it should look something like this:\n\n```python\nfrom pyspark.sql.functions import desc_nulls_first\ndf.orderBy(col(\"Description\").desc_nulls_first()).show()\n```\n\nHowever, its Sparl SQL function works fine:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 65, "cell_type": "code", "source": "spark.sql(\"\"\"\nSELECT * FROM dfTable\nORDER BY Description DESC NULLS FIRST\nLIMIT 15\n\"\"\").show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536545|    21134|                null|       1|2010-12-01 14:32:00|      0.0|      null|United Kingdom|\n|   536414|    22139|                null|      56|2010-12-01 11:52:00|      0.0|      null|United Kingdom|\n|   536546|    22145|                null|       1|2010-12-01 14:33:00|      0.0|      null|United Kingdom|\n|   536547|    37509|                null|       1|2010-12-01 14:33:00|      0.0|      null|United Kingdom|\n|   536549|   85226A|                null|       1|2010-12-01 14:34:00|      0.0|      null|United Kingdom|\n|   536550|    85044|                null|       1|2010-12-01 14:34:00|      0.0|      null|United Kingdom|\n|   536552|    20950|                null|       1|2010-12-01 14:34:00|      0.0|      null|United Kingdom|\n|   536553|    37461|                null|       3|2010-12-01 14:35:00|      0.0|      null|United Kingdom|\n|   536554|    84670|                null|      23|2010-12-01 14:35:00|      0.0|      null|United Kingdom|\n|   536589|    21777|                null|     -10|2010-12-01 16:50:00|      0.0|      null|United Kingdom|\n|   536592|    84832|ZINC WILLIE WINKI...|       1|2010-12-01 17:06:00|     1.66|      null|United Kingdom|\n|   536381|    84832|ZINC WILLIE WINKI...|       1|2010-12-01 09:41:00|     0.85|   15311.0|United Kingdom|\n|   536544|    84832|ZINC WILLIE WINKI...|       3|2010-12-01 14:32:00|     1.66|      null|United Kingdom|\n|   536446|    84836|ZINC METAL HEART ...|      12|2010-12-01 12:15:00|     1.25|   15983.0|United Kingdom|\n|   536570|    84836|ZINC METAL HEART ...|      12|2010-12-01 15:35:00|     1.25|   14496.0|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n\n"}], "metadata": {}}, {"source": "## Working with Complex Types\n\nComplex types can help you organize and structure your data in ways that make more sense for the problem that you are hoping to solve. There are three kinds of complex types: structs, arrays, and maps.\n\n### Structs\n\nYou can think of structs as DataFrames within DataFrames. A worked example will illustrate this more clearly. We can create a struct by wrapping a set of columns in parenthesis in a query:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 66, "cell_type": "code", "source": "df.selectExpr(\"(Description, InvoiceNo) as complex\", \"*\").show(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|             complex|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+--------------------+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|[WHITE HANGING HE...|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n|[WHITE METAL LANT...|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n|[CREAM CUPID HEAR...|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n|[KNITTED UNION FL...|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n|[RED WOOLLY HOTTI...|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n+--------------------+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 5 rows\n\n"}], "metadata": {}}, {"source": "Which is equivalent to:\n\n```python\ndf.selectExpr(\"struct(Description, InvoiceNo) as complex\", \"*\")\n```\n\nLet's make a new DataFrame by wrapping two columns, and including two other columns from our df:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 67, "cell_type": "code", "source": "from pyspark.sql.functions import struct\n\ncomplexDF = df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"), \"StockCode\", \"CustomerID\")\ncomplexDF.createOrReplaceTempView(\"complexDF\")", "outputs": [], "metadata": {}}, {"execution_count": 68, "cell_type": "code", "source": "complexDF.show(5, False)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---------------------------------------------+---------+----------+\n|complex                                      |StockCode|CustomerID|\n+---------------------------------------------+---------+----------+\n|[WHITE HANGING HEART T-LIGHT HOLDER, 536365] |85123A   |17850.0   |\n|[WHITE METAL LANTERN, 536365]                |71053    |17850.0   |\n|[CREAM CUPID HEARTS COAT HANGER, 536365]     |84406B   |17850.0   |\n|[KNITTED UNION FLAG HOT WATER BOTTLE, 536365]|84029G   |17850.0   |\n|[RED WOOLLY HOTTIE WHITE HEART., 536365]     |84029E   |17850.0   |\n+---------------------------------------------+---------+----------+\nonly showing top 5 rows\n\n"}], "metadata": {}}, {"source": "We now have a DataFrame with a column complex. We can query it just as we might another DataFrame, the only difference is that we use a dot syntax to do so, or the column method `getField`:\n\nWe can access and expand all the columns by:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 69, "cell_type": "code", "source": "complexDF.select(\"complex.Description\", \"complex.InvoiceNo\", \"StockCode\", \"CustomerID\").show(5, False)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----------------------------------+---------+---------+----------+\n|Description                        |InvoiceNo|StockCode|CustomerID|\n+-----------------------------------+---------+---------+----------+\n|WHITE HANGING HEART T-LIGHT HOLDER |536365   |85123A   |17850.0   |\n|WHITE METAL LANTERN                |536365   |71053    |17850.0   |\n|CREAM CUPID HEARTS COAT HANGER     |536365   |84406B   |17850.0   |\n|KNITTED UNION FLAG HOT WATER BOTTLE|536365   |84029G   |17850.0   |\n|RED WOOLLY HOTTIE WHITE HEART.     |536365   |84029E   |17850.0   |\n+-----------------------------------+---------+---------+----------+\nonly showing top 5 rows\n\n"}], "metadata": {}}, {"execution_count": 70, "cell_type": "code", "source": "complexDF.select(col(\"complex\").getField(\"Description\")).show(2)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+\n| complex.Description|\n+--------------------+\n|WHITE HANGING HEA...|\n| WHITE METAL LANTERN|\n+--------------------+\nonly showing top 2 rows\n\n"}], "metadata": {}}, {"execution_count": 71, "cell_type": "code", "source": "complexDF.printSchema()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- complex: struct (nullable = false)\n |    |-- Description: string (nullable = true)\n |    |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- CustomerID: double (nullable = true)\n\n"}], "metadata": {}}, {"source": "We can also query all values in the struct by using *. This brings up all the columns to the top-level DataFrame:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 72, "cell_type": "code", "source": "complexDF.select(\"complex.*\").show(2)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+---------+\n|         Description|InvoiceNo|\n+--------------------+---------+\n|WHITE HANGING HEA...|   536365|\n| WHITE METAL LANTERN|   536365|\n+--------------------+---------+\nonly showing top 2 rows\n\n"}], "metadata": {}}, {"source": "In SQL\n```sql\nSELECT complex.* FROM complexDF\n```", "cell_type": "markdown", "metadata": {}}, {"source": "### Arrays\n\nTo define arrays, let\u2019s work through a use case. With our current data, our objective is to take every single word in our Description column and convert that into a row in our DataFrame.\n\nThe first task is to turn our Description column into a complex type, an array.\n\n### `split`\n\nWe do this by using the split function and specify the delimiter:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 73, "cell_type": "code", "source": "from pyspark.sql.functions import split\n\ndf.select(split(col(\"Description\"), \" \")).show(2)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---------------------+\n|split(Description,  )|\n+---------------------+\n| [WHITE, HANGING, ...|\n| [WHITE, METAL, LA...|\n+---------------------+\nonly showing top 2 rows\n\n"}], "metadata": {}}, {"source": "In SQL\n```sql\nSELECT split(Description, ' ') FROM dfTable\n```\n\nThis is quite powerful because Spark allows us to manipulate this complex type as another column. We can also query the values of the array using Python-like syntax:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 74, "cell_type": "code", "source": "df.select(split(col(\"Description\"), \" \").alias(\"array_col\"))\\\n  .selectExpr(\"array_col[0]\").show(2)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+------------+\n|array_col[0]|\n+------------+\n|       WHITE|\n|       WHITE|\n+------------+\nonly showing top 2 rows\n\n"}], "metadata": {}}, {"source": "In SQL\n```sql\nSELECT split(Description, ' ')[0] FROM dfTable\n```\n\n### Array Length\n\nWe can determine the array\u2019s length by querying for its size:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 75, "cell_type": "code", "source": "from pyspark.sql.functions import size\n\ndf.select(size(split(col(\"Description\"), \" \"))).show(2)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---------------------------+\n|size(split(Description,  ))|\n+---------------------------+\n|                          5|\n|                          3|\n+---------------------------+\nonly showing top 2 rows\n\n"}], "metadata": {}}, {"source": "### `array_contains`\n\nWe can also see whether this array contains a value:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 76, "cell_type": "code", "source": "from pyspark.sql.functions import array_contains\n\ndf.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\")).show(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------------------------------+\n|array_contains(split(Description,  ), WHITE)|\n+--------------------------------------------+\n|                                        true|\n|                                        true|\n|                                       false|\n|                                       false|\n|                                        true|\n+--------------------------------------------+\nonly showing top 5 rows\n\n"}], "metadata": {}}, {"source": "However, this does not solve our current problem. To convert a complex type into a set of rows (one per value in our array), we need to use the explode function.\n\n### `explode`\n\nThe `explode` function takes a column that consists of arrays and creates one row (with the rest of the values duplicated) per value in the array. Figure below illustrates the process.\n\n<img src=\"https://github.com/soltaniehha/Big-Data-Analytics-for-Business/blob/master/figs/06-01-Exploding-a-column-of-text.png?raw=true\" width=\"900\" align=\"left\"/>", "cell_type": "markdown", "metadata": {}}, {"execution_count": 77, "cell_type": "code", "source": "from pyspark.sql.functions import split, explode\n\ndf.withColumn(\"splitted\", split(col(\"Description\"), \" \"))\\\n  .withColumn(\"exploded\", explode(col(\"splitted\")))\\\n  .select(\"Description\", \"InvoiceNo\", \"exploded\").show(13, False)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------------------------------+---------+--------+\n|Description                       |InvoiceNo|exploded|\n+----------------------------------+---------+--------+\n|WHITE HANGING HEART T-LIGHT HOLDER|536365   |WHITE   |\n|WHITE HANGING HEART T-LIGHT HOLDER|536365   |HANGING |\n|WHITE HANGING HEART T-LIGHT HOLDER|536365   |HEART   |\n|WHITE HANGING HEART T-LIGHT HOLDER|536365   |T-LIGHT |\n|WHITE HANGING HEART T-LIGHT HOLDER|536365   |HOLDER  |\n|WHITE METAL LANTERN               |536365   |WHITE   |\n|WHITE METAL LANTERN               |536365   |METAL   |\n|WHITE METAL LANTERN               |536365   |LANTERN |\n|CREAM CUPID HEARTS COAT HANGER    |536365   |CREAM   |\n|CREAM CUPID HEARTS COAT HANGER    |536365   |CUPID   |\n|CREAM CUPID HEARTS COAT HANGER    |536365   |HEARTS  |\n|CREAM CUPID HEARTS COAT HANGER    |536365   |COAT    |\n|CREAM CUPID HEARTS COAT HANGER    |536365   |HANGER  |\n+----------------------------------+---------+--------+\nonly showing top 13 rows\n\n"}], "metadata": {}}, {"source": "### Maps\n\nMaps are created by using the `create_map` function (`map` in SQL) and key-value pairs of columns. You then can select them just like you might select from an array:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 78, "cell_type": "code", "source": "from pyspark.sql.functions import create_map\n\ndf.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\")).show(5, False)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----------------------------------------------+\n|complex_map                                    |\n+-----------------------------------------------+\n|[WHITE HANGING HEART T-LIGHT HOLDER -> 536365] |\n|[WHITE METAL LANTERN -> 536365]                |\n|[CREAM CUPID HEARTS COAT HANGER -> 536365]     |\n|[KNITTED UNION FLAG HOT WATER BOTTLE -> 536365]|\n|[RED WOOLLY HOTTIE WHITE HEART. -> 536365]     |\n+-----------------------------------------------+\nonly showing top 5 rows\n\n"}], "metadata": {}}, {"source": "In SQL\n```sql\nSELECT map(Description, InvoiceNo) as complex_map FROM dfTable\nWHERE Description IS NOT NULL\n```", "cell_type": "markdown", "metadata": {}}, {"execution_count": 79, "cell_type": "code", "source": "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n  .selectExpr(\"*\", \"complex_map['WHITE METAL LANTERN']\").show(5, False)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----------------------------------------------+--------------------------------+\n|complex_map                                    |complex_map[WHITE METAL LANTERN]|\n+-----------------------------------------------+--------------------------------+\n|[WHITE HANGING HEART T-LIGHT HOLDER -> 536365] |null                            |\n|[WHITE METAL LANTERN -> 536365]                |536365                          |\n|[CREAM CUPID HEARTS COAT HANGER -> 536365]     |null                            |\n|[KNITTED UNION FLAG HOT WATER BOTTLE -> 536365]|null                            |\n|[RED WOOLLY HOTTIE WHITE HEART. -> 536365]     |null                            |\n+-----------------------------------------------+--------------------------------+\nonly showing top 5 rows\n\n"}], "metadata": {}}, {"source": "You can also explode map types, which will turn them into columns:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 80, "cell_type": "code", "source": "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n  .selectExpr(\"explode(complex_map)\").show(2, False)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------------------------------+------+\n|key                               |value |\n+----------------------------------+------+\n|WHITE HANGING HEART T-LIGHT HOLDER|536365|\n|WHITE METAL LANTERN               |536365|\n+----------------------------------+------+\nonly showing top 2 rows\n\n"}], "metadata": {}}, {"source": "## User-Defined Functions\n\nOne of the most powerful things that you can do in Spark is define your own functions. These user-defined functions (UDFs) make it possible for you to write your own custom transformations using Python or Scala and even use external libraries. UDFs can take and return one or more columns as input. Spark UDFs are incredibly powerful because you can write them in several different programming languages; you do not need to create them in an esoteric format or domain-specific language. They\u2019re just functions that operate on the data, record by record. By default, these functions are registered as temporary functions to be used in that specific SparkSession or Context.\n\nAlthough you can write UDFs in Scala, Python, or Java, there are performance considerations that you should be aware of. To illustrate this, we\u2019re going to walk through exactly what happens when you create UDF, pass that into Spark, and then execute code using that UDF.\n\nThe first step is the actual function. We\u2019ll create a simple one for this example. Let\u2019s write a power3 function that takes a number and raises it to a power of three:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 81, "cell_type": "code", "source": "udfExampleDF = spark.range(5).toDF(\"num\")\nudfExampleDF.createOrReplaceTempView(\"udfExampleDFTable\")\n\ndef power3(double_value):\n    return double_value ** 3\n\npower3(2.0)", "outputs": [{"execution_count": 81, "output_type": "execute_result", "data": {"text/plain": "8.0"}, "metadata": {}}], "metadata": {}}, {"source": "In this trivial example, we can see that our functions work as expected. We are able to provide an individual input and produce the expected result (with this simple test case). Thus far, our expectations for the input are high: it must be a specific type and cannot be a null value.\n\nNow that we\u2019ve created these functions and tested them, we need to register them with Spark so that we can use them on all of our worker machines. Spark will serialize the function on the driver and transfer it over the network to all executor processes. This happens regardless of language.\n\nWhen you use the function, Spark starts a Python process on the worker, serializes all of the data to a format that Python can understand (remember, it was in the JVM earlier), executes the function row by row on that data in the Python process, and then finally returns the results of the row operations to the JVM and Spark. Figure below provides an overview of the process:\n\n<img src=\"https://github.com/soltaniehha/Big-Data-Analytics-for-Business/blob/master/figs/06-01-UDF.png?raw=true\" width=\"700\" align=\"center\"/>\n\n**Warning:** Starting this Python process is expensive. The real cost is in serializing the data to Python.\n\nWe need to register the function to make it available as a DataFrame function:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 82, "cell_type": "code", "source": "from pyspark.sql.functions import udf\n\npower3udf = udf(power3)", "outputs": [], "metadata": {}}, {"source": "Then, we can use it in our DataFrame code:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 83, "cell_type": "code", "source": "from pyspark.sql.functions import col\n\nudfExampleDF.select(power3udf(col(\"num\"))).show(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----------+\n|power3(num)|\n+-----------+\n|          0|\n|          1|\n|          8|\n|         27|\n|         64|\n+-----------+\n\n"}], "metadata": {}}, {"source": "At this juncture, we can use this only as a DataFrame function. That is to say, we can\u2019t use it within a string expression, only on an expression. However, we can also register this UDF as a Spark SQL function. This is valuable because it makes it simple to use this function within SQL as well as across languages.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 84, "cell_type": "code", "source": "spark.udf.register(\"power3py\", power3)", "outputs": [{"execution_count": 84, "output_type": "execute_result", "data": {"text/plain": "<function __main__.power3py>"}, "metadata": {}}], "metadata": {}}, {"execution_count": 85, "cell_type": "code", "source": "udfExampleDF.selectExpr(\"power3py(num)\").show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------------+\n|power3py(num)|\n+-------------+\n|            0|\n|            1|\n|            8|\n|           27|\n|           64|\n+-------------+\n\n"}], "metadata": {}}, {"source": "In SQL:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 86, "cell_type": "code", "source": "spark.sql(\"\"\"\nSELECT num, power3py(num) from udfExampleDFTable\n\"\"\").show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---+-------------+\n|num|power3py(num)|\n+---+-------------+\n|  0|            0|\n|  1|            1|\n|  2|            8|\n|  3|           27|\n|  4|           64|\n+---+-------------+\n\n"}], "metadata": {}}, {"source": "For a complete list of **pyspark.sql.functions** visit [Spark 2.4 documentation page](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#module-pyspark.sql.functions).", "cell_type": "markdown", "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pyspark", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.14", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}, "name": "06_Working_with_Different_Types_of_Data", "notebookId": 3218934446332021}}