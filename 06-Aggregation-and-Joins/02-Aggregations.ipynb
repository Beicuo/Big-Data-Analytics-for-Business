{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregations\n",
    "\n",
    "Spark allows us to create several different grouping types for aggregation. This notebook will discuss some of these grouping and aggregation techniques.\n",
    "\n",
    "Let's first load some retail data. We have over 300 csv files each representing daily transactions in a retail store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"gs://is843/notebooks/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(data + \"retail-data/all/*.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s a sample of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate   |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+\n",
      "|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER |6       |12/1/2010 8:26|2.55     |17850     |United Kingdom|\n",
      "|536365   |71053    |WHITE METAL LANTERN                |6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|\n",
      "|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER     |8       |12/1/2010 8:26|2.75     |17850     |United Kingdom|\n",
      "|536365   |84029G   |KNITTED UNION FLAG HOT WATER BOTTLE|6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|\n",
      "|536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.     |6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|\n",
      "+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5, False)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`InvoiceDate` is being recognized as string. Let's replace it with a date format. We won't need the timestamp, so it's ok to lose it.\n",
    "\n",
    "We will also cast `CustomerID` as string and Quantity as long:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6| 2010-12-01|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6| 2010-12-01|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8| 2010-12-01|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6| 2010-12-01|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6| 2010-12-01|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: long (nullable = true)\n",
      " |-- InvoiceDate: date (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "df = df.withColumn(\"InvoiceDate\", to_date(col(\"InvoiceDate\"), \"MM/d/yyyy H:mm\"))\n",
    "\n",
    "df = df.withColumn(\"CustomerID\", col(\"CustomerID\").cast(\"string\"))\n",
    "df = df.withColumn(\"Quantity\", col(\"Quantity\").cast(\"long\"))\n",
    "\n",
    "df.createOrReplaceTempView(\"dfTable\")\n",
    "\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching\n",
    "\n",
    "Caching allows the DataFrame to be loaded and persist in the memory. If we don't use this option, every time we execute an action our DataFrame gets loaded from our Cloud Storage, which is not ideal and will add to our execution time:\n",
    "\n",
    "**Note:** Caching is a lazy transformation. It will happen the first time you execute an action against the DataFrame, not when you cache that DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: bigint, InvoiceDate: date, UnitPrice: double, CustomerID: string, Country: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic aggregations apply to an entire DataFrame. The simplest example is the `count` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It returns the total number of rows in this DataFrame. `count` when used in this context is actually an action, so it returns the output immediately. We will also encounter cases that `count` will be acting as a lazy transformation.\n",
    "\n",
    "Now that we have performed an action on `df` it should be cached into the memory. Go ahead an execute the previous command again to see the performance gain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once done with the DataFrame we can free up the memory by removing the cache. This can be done by `unpersist`:\n",
    "```python\n",
    "df.unpersist()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation Functions\n",
    "\n",
    "All aggregations are available as functions, in addition to the special cases that can appear on DataFrames or via `.stat`. You can find most aggregation functions in the `pyspark.sql.functions` package.\n",
    "\n",
    "**Note:** There are some gaps between the available SQL functions and the functions that we can import in Scala and Python. This changes every release, so it’s impossible to include a definitive list. This section covers the most common functions.\n",
    "\n",
    "### `count`\n",
    "The first function worth going over is `count`, except in this example it will perform as a transformation instead of an action. In this case, we can do one of two things: specify a specific column to count, or all the columns by using `count(*)` or `count(1)` to represent that we want to count every row as the literal one, as shown in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(StockCode)|\n",
      "+----------------+\n",
      "|          541909|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "df.select(count(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SQL\n",
    "```sql\n",
    "SELECT COUNT(*) FROM dfTable\n",
    "```\n",
    "\n",
    "**Warning**\n",
    "\n",
    "when performing a `count(*)`, Spark will count null values (including rows containing all nulls). However, when counting an individual column, Spark will not count the null values. For instance if we repeate this for \"CustomerID\" column we will get a different value due to the null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|count(CustomerID)|\n",
      "+-----------------+\n",
      "|           406829|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(count(\"CustomerID\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `countDistinct`\n",
    "\n",
    "Sometimes, the total number is not relevant; rather, it’s the number of unique groups that you want. To get this number, you can use the `countDistinct` function. This is a bit more relevant for individual columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "df.select(countDistinct(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SQL\n",
    "```sql\n",
    "SELECT COUNT(DISTINCT *) FROM DFTABLE\n",
    "```\n",
    "\n",
    "`approx_count_distinct`: This function can be used to estimate the count distinct. It will give us a lot of performace gain.\n",
    "\n",
    "### `min` and `max`\n",
    "\n",
    "To extract the minimum and maximum values from a DataFrame, use the min and max functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|       -80995|        80995|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "df.select(min(\"Quantity\"), max(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sum`\n",
    "\n",
    "Another simple task is to add all the values in a row using the sum function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "df.select(sum(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `avg`\n",
    "\n",
    "`avg` or `mean` functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+----------------+----------------+\n",
      "|(total_purchases / total_transactions)|   avg_purchases|  mean_purchases|\n",
      "+--------------------------------------+----------------+----------------+\n",
      "|                      9.55224954743324|9.55224954743324|9.55224954743324|\n",
      "+--------------------------------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, avg, expr\n",
    "\n",
    "df.select(\n",
    "    count(\"Quantity\").alias(\"total_transactions\"),\n",
    "    sum(\"Quantity\").alias(\"total_purchases\"),\n",
    "    avg(\"Quantity\").alias(\"avg_purchases\"),\n",
    "    expr(\"mean(Quantity)\").alias(\"mean_purchases\"))\\\n",
    "  .selectExpr(\n",
    "    \"total_purchases/total_transactions\",\n",
    "    \"avg_purchases\",\n",
    "    \"mean_purchases\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance and Standard Deviation\n",
    "\n",
    "Calculating the mean naturally brings up questions about the variance and standard deviation. These are both measures of the spread of the data around the mean. The variance is the average of the squared differences from the mean, and the standard deviation is the square root of the variance. You can calculate these in Spark by using their respective functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------------+\n",
      "|var_samp(Quantity)|stddev_samp(Quantity)|\n",
      "+------------------+---------------------+\n",
      "| 47559.39140929905|   218.08115785023486|\n",
      "+------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import variance, stddev\n",
    "\n",
    "df.select(variance(\"Quantity\"), stddev(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something to note is that Spark has both the formula for the sample standard deviation as well as the formula for the population standard deviation. These are fundamentally different statistical formulae, and we need to differentiate between them. By default, Spark performs the formula for the sample standard deviation or variance if you use the `variance` or `stddev` functions.\n",
    "\n",
    "You can also specify these explicitly or refer to the population standard deviation or variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------+---------------------+\n",
      "| var_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|stddev_samp(Quantity)|\n",
      "+------------------+------------------+--------------------+---------------------+\n",
      "|47559.303646609354| 47559.39140929905|  218.08095663447864|   218.08115785023486|\n",
      "+------------------+------------------+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import var_pop, stddev_pop\n",
    "from pyspark.sql.functions import var_samp, stddev_samp\n",
    "\n",
    "df.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"),\n",
    "  stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SQL\n",
    "```sql\n",
    "SELECT var_pop(Quantity), var_samp(Quantity),\n",
    "  stddev_pop(Quantity), stddev_samp(Quantity)\n",
    "FROM dfTable\n",
    "```\n",
    "\n",
    "### Covariance and Correlation\n",
    "\n",
    "We discussed single column aggregations, but some functions compare the interactions of the values in two difference columns together. Two of these functions are `covar_samp` and `corr`, for covariance and correlation, respectively. Correlation measures the Pearson correlation coefficient, which is scaled between –1 and +1. The covariance is scaled according to the inputs in the data. There also exists a covariance function for the population, covar_pop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------------------+\n",
      "|corr(UnitPrice, Quantity)|covar_samp(UnitPrice, Quantity)|\n",
      "+-------------------------+-------------------------------+\n",
      "|     -0.00123492454487...|            -26.058761257936965|\n",
      "+-------------------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr, covar_samp\n",
    "\n",
    "df.select(corr(\"UnitPrice\", \"Quantity\"), covar_samp(\"UnitPrice\", \"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping\n",
    "\n",
    "Thus far, we have performed only DataFrame-level aggregations. A more common task is to perform calculations based on groups in the data. This is typically done on categorical data for which we group our data on one column and perform some calculations on the other columns that end up in that group.\n",
    "\n",
    "The best way to explain this is to begin performing some groupings. The first will be a count, just as we did before. We will group by each unique invoice number and get the count of items on that invoice. Note that this returns another DataFrame and is lazily performed.\n",
    "\n",
    "We do this grouping in two phases. First we specify the column(s) on which we would like to group, and then we specify the aggregation(s). The first step returns a `RelationalGroupedDataset`, and the second step returns a `DataFrame`.\n",
    "\n",
    "As mentioned, we can specify any number of columns on which we want to group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x7f5b5c166eb8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\", \"CustomerId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|InvoiceNo|CustomerId|count|\n",
      "+---------+----------+-----+\n",
      "|   563557|     15786|  166|\n",
      "|   563945|     17581|    4|\n",
      "|   564078|     17083|   18|\n",
      "|   564325|     13694|    2|\n",
      "|   564842|     16133|   11|\n",
      "+---------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SQL\n",
    "```sql\n",
    "SELECT count(*) FROM dfTable GROUP BY InvoiceNo, CustomerId\n",
    "```\n",
    "\n",
    "## Grouping with Expressions\n",
    "\n",
    "As we saw earlier, counting is a bit of a special case because it exists as a method. For this, usually we prefer to use the `count` function. Rather than passing that function as an expression into a `select` statement, we specify it as within `agg`. This makes it possible for you to pass-in arbitrary expressions that just need to have some aggregation specified. You can even do things like `alias` a column after transforming it for later use in your data flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------------+\n",
      "|InvoiceNo|quan|count(Quantity)|\n",
      "+---------+----+---------------+\n",
      "|   563020|  24|             24|\n",
      "|   565747|  10|             10|\n",
      "|   566248|   8|              8|\n",
      "|   566431|  18|             18|\n",
      "|   567163|  14|             14|\n",
      "+---------+----+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "df.groupBy(\"InvoiceNo\")\\\n",
    "  .agg(count(\"Quantity\").alias(\"quan\"),\n",
    "       expr(\"count(Quantity)\")\n",
    "      ).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+--------------------+\n",
      "|InvoiceNo|     avg(Quantity)|stddev_pop(Quantity)|\n",
      "+---------+------------------+--------------------+\n",
      "|   536596|               1.5|  1.1180339887498947|\n",
      "|   536938|33.142857142857146|  20.698023172885524|\n",
      "|   537252|              31.0|                 0.0|\n",
      "|   537691|              8.15|   5.597097462078001|\n",
      "|   538041|              30.0|                 0.0|\n",
      "+---------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\").agg(expr(\"avg(Quantity)\"),expr(\"stddev_pop(Quantity)\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window Functions\n",
    "\n",
    "Window functions operate on a set of rows and return a single value for each row from the underlying query. The term window describes the set of rows on which the function operates. A window function uses values from the rows in a window to calculate the returned values.\n",
    "\n",
    "The following SQL query adds a new colummn (`overall_dataset_Q`) that includes the overall quantity for the entire dataset. This value will be the same for all the rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------+-----------------+\n",
      "|CustomerId|InvoiceDate|Quantity|overall_dataset_Q|\n",
      "+----------+-----------+--------+-----------------+\n",
      "|     12346| 2011-01-18|  -74215|          4906888|\n",
      "|     12346| 2011-01-18|   74215|          4906888|\n",
      "|     12347| 2011-12-07|      12|          4906888|\n",
      "|     12347| 2011-12-07|      24|          4906888|\n",
      "|     12347| 2011-12-07|      24|          4906888|\n",
      "+----------+-----------+--------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT CustomerId, InvoiceDate, Quantity,\n",
    "\n",
    "  sum(Quantity) OVER () as overall_dataset_Q\n",
    "  \n",
    "FROM dfTable WHERE CustomerId IS NOT NULL \n",
    "ORDER BY CustomerId, InvoiceDate DESC\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`OVER()` clause indicates that we want to use a window over the `sum()` expression. \n",
    "\n",
    "The `OVER()` clause has the following capabilities:\n",
    "\n",
    "* Defines window partitions to form groups of rows. (`PARTITION BY` clause)\n",
    "* Orders rows within a partition. (`ORDER BY` clause)\n",
    "\n",
    "In the above query since we haven't defined a `PARTITION` within `OVER()` the aggregating function applies to the entire dataset.\n",
    "\n",
    "We can do a lot more useful calculations with the window functions. For instance, we can add a total quantity per customer and a total quantity per customer by date, specifying the appropriate `PARTITION`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------+-----------------+-------+---------------+\n",
      "|CustomerId|InvoiceDate|Quantity|overall_dataset_Q|total_Q|total_Q_by_date|\n",
      "+----------+-----------+--------+-----------------+-------+---------------+\n",
      "|     12346| 2011-01-18|   74215|          4906888|      0|              0|\n",
      "|     12346| 2011-01-18|  -74215|          4906888|      0|              0|\n",
      "|     12347| 2011-12-07|      20|          4906888|   2458|            192|\n",
      "|     12347| 2011-12-07|      12|          4906888|   2458|            192|\n",
      "|     12347| 2011-12-07|       6|          4906888|   2458|            192|\n",
      "|     12347| 2011-12-07|      16|          4906888|   2458|            192|\n",
      "|     12347| 2011-12-07|      20|          4906888|   2458|            192|\n",
      "|     12347| 2011-12-07|      24|          4906888|   2458|            192|\n",
      "|     12347| 2011-12-07|      24|          4906888|   2458|            192|\n",
      "|     12347| 2011-12-07|      24|          4906888|   2458|            192|\n",
      "|     12347| 2011-12-07|      10|          4906888|   2458|            192|\n",
      "|     12347| 2011-12-07|      12|          4906888|   2458|            192|\n",
      "|     12347| 2011-12-07|      24|          4906888|   2458|            192|\n",
      "|     12347| 2011-10-31|       4|          4906888|   2458|            676|\n",
      "|     12347| 2011-10-31|       8|          4906888|   2458|            676|\n",
      "|     12347| 2011-10-31|      12|          4906888|   2458|            676|\n",
      "|     12347| 2011-10-31|      12|          4906888|   2458|            676|\n",
      "|     12347| 2011-10-31|      20|          4906888|   2458|            676|\n",
      "|     12347| 2011-10-31|      12|          4906888|   2458|            676|\n",
      "|     12347| 2011-10-31|      12|          4906888|   2458|            676|\n",
      "+----------+-----------+--------+-----------------+-------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT CustomerId, InvoiceDate, Quantity,\n",
    "\n",
    "  sum(Quantity) OVER () as overall_dataset_Q,\n",
    "  \n",
    "  sum(Quantity) OVER (PARTITION BY CustomerId) as total_Q,\n",
    "  \n",
    "  sum(Quantity) OVER (PARTITION BY CustomerId, InvoiceDate) as total_Q_by_date\n",
    "  \n",
    "FROM dfTable WHERE CustomerId IS NOT NULL \n",
    "ORDER BY CustomerId, InvoiceDate DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `ORDER BY` clause we can sort the rows within the `PARTITION`s and then use the `rank()` function to give them a ranking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------+-----------------+-------+---------------+----+\n",
      "|CustomerId|InvoiceDate|Quantity|overall_dataset_Q|total_Q|total_Q_by_date|rank|\n",
      "+----------+-----------+--------+-----------------+-------+---------------+----+\n",
      "|     12346| 2011-01-18|   74215|          4906888|      0|              0|   1|\n",
      "|     12346| 2011-01-18|  -74215|          4906888|      0|              0|   2|\n",
      "|     12347| 2011-12-07|      24|          4906888|   2458|            192|   1|\n",
      "|     12347| 2011-12-07|      24|          4906888|   2458|            192|   1|\n",
      "|     12347| 2011-12-07|      24|          4906888|   2458|            192|   1|\n",
      "|     12347| 2011-12-07|      24|          4906888|   2458|            192|   1|\n",
      "|     12347| 2011-12-07|      20|          4906888|   2458|            192|   5|\n",
      "|     12347| 2011-12-07|      20|          4906888|   2458|            192|   5|\n",
      "|     12347| 2011-12-07|      16|          4906888|   2458|            192|   7|\n",
      "|     12347| 2011-12-07|      12|          4906888|   2458|            192|   8|\n",
      "|     12347| 2011-12-07|      12|          4906888|   2458|            192|   8|\n",
      "|     12347| 2011-12-07|      10|          4906888|   2458|            192|  10|\n",
      "|     12347| 2011-12-07|       6|          4906888|   2458|            192|  11|\n",
      "|     12347| 2011-10-31|      48|          4906888|   2458|            676|   1|\n",
      "|     12347| 2011-10-31|      36|          4906888|   2458|            676|   2|\n",
      "|     12347| 2011-10-31|      36|          4906888|   2458|            676|   2|\n",
      "|     12347| 2011-10-31|      24|          4906888|   2458|            676|   4|\n",
      "|     12347| 2011-10-31|      24|          4906888|   2458|            676|   4|\n",
      "|     12347| 2011-10-31|      24|          4906888|   2458|            676|   4|\n",
      "|     12347| 2011-10-31|      24|          4906888|   2458|            676|   4|\n",
      "+----------+-----------+--------+-----------------+-------+---------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT CustomerId, InvoiceDate, Quantity,\n",
    "\n",
    "  sum(Quantity) OVER () as overall_dataset_Q,\n",
    "  \n",
    "  sum(Quantity) OVER (PARTITION BY CustomerId) as total_Q,\n",
    "  \n",
    "  sum(Quantity) OVER (PARTITION BY CustomerId, InvoiceDate) as total_Q_by_date,\n",
    "  \n",
    "  RANK() OVER (PARTITION BY CustomerId, InvoiceDate \n",
    "               ORDER BY Quantity DESC) as rank\n",
    "  \n",
    "FROM dfTable WHERE CustomerId IS NOT NULL \n",
    "ORDER BY CustomerId, InvoiceDate DESC, rank\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please notice the behavior of `rank()` when it comes across tied values.\n",
    "\n",
    "`dense_rank()` returns the rank of rows within a window partition, without any gaps.\n",
    "\n",
    "`row_number()` returns a sequential number starting at 1 within a window partition. Please note that the ties will be numbered at random. This could have downstream consequences!\n",
    "\n",
    "In the example below we have added both `dense_rank()` and `row_number()` to the previous query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------+-----------------+-------+-----+---------------+----+------+----------+\n",
      "|CustomerId|InvoiceDate|Quantity|overall_dataset_Q|total_Q|max_Q|total_Q_by_date|rank|d_rank|row_number|\n",
      "+----------+-----------+--------+-----------------+-------+-----+---------------+----+------+----------+\n",
      "|     12346| 2011-01-18|   74215|          4906888|      0|74215|              0|   1|     1|         1|\n",
      "|     12346| 2011-01-18|  -74215|          4906888|      0|74215|              0|   2|     2|         2|\n",
      "|     12347| 2011-12-07|      24|          4906888|   2458|  240|            192|   1|     1|         1|\n",
      "|     12347| 2011-12-07|      24|          4906888|   2458|  240|            192|   1|     1|         2|\n",
      "|     12347| 2011-12-07|      24|          4906888|   2458|  240|            192|   1|     1|         3|\n",
      "|     12347| 2011-12-07|      24|          4906888|   2458|  240|            192|   1|     1|         4|\n",
      "|     12347| 2011-12-07|      20|          4906888|   2458|  240|            192|   5|     2|         5|\n",
      "|     12347| 2011-12-07|      20|          4906888|   2458|  240|            192|   5|     2|         6|\n",
      "|     12347| 2011-12-07|      16|          4906888|   2458|  240|            192|   7|     3|         7|\n",
      "|     12347| 2011-12-07|      12|          4906888|   2458|  240|            192|   8|     4|         8|\n",
      "|     12347| 2011-12-07|      12|          4906888|   2458|  240|            192|   8|     4|         9|\n",
      "|     12347| 2011-12-07|      10|          4906888|   2458|  240|            192|  10|     5|        10|\n",
      "|     12347| 2011-12-07|       6|          4906888|   2458|  240|            192|  11|     6|        11|\n",
      "|     12347| 2011-10-31|      48|          4906888|   2458|  240|            676|   1|     1|         1|\n",
      "|     12347| 2011-10-31|      36|          4906888|   2458|  240|            676|   2|     2|         2|\n",
      "|     12347| 2011-10-31|      36|          4906888|   2458|  240|            676|   2|     2|         3|\n",
      "|     12347| 2011-10-31|      24|          4906888|   2458|  240|            676|   4|     3|         4|\n",
      "|     12347| 2011-10-31|      24|          4906888|   2458|  240|            676|   4|     3|         5|\n",
      "|     12347| 2011-10-31|      24|          4906888|   2458|  240|            676|   4|     3|         6|\n",
      "|     12347| 2011-10-31|      24|          4906888|   2458|  240|            676|   4|     3|         7|\n",
      "+----------+-----------+--------+-----------------+-------+-----+---------------+----+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT CustomerId, InvoiceDate, Quantity,\n",
    "\n",
    "  sum(Quantity) OVER () as overall_dataset_Q,\n",
    "  \n",
    "  sum(Quantity) OVER (PARTITION BY CustomerId) as total_Q,\n",
    "  \n",
    "  max(Quantity) OVER (PARTITION BY CustomerId) as max_Q,\n",
    "\n",
    "  sum(Quantity) OVER (PARTITION BY CustomerId, InvoiceDate) as total_Q_by_date,\n",
    "    \n",
    "  RANK() OVER (PARTITION BY CustomerId, InvoiceDate \n",
    "               ORDER BY Quantity DESC) as rank,\n",
    "  \n",
    "  DENSE_RANK() OVER (PARTITION BY CustomerId, InvoiceDate \n",
    "                     ORDER BY Quantity DESC) as d_rank,\n",
    "  \n",
    "  ROW_NUMBER() OVER (PARTITION BY CustomerId, InvoiceDate \n",
    "                     ORDER BY Quantity DESC) as row_number\n",
    "  \n",
    "FROM dfTable WHERE CustomerId IS NOT NULL \n",
    "ORDER BY CustomerId, InvoiceDate DESC, row_number\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few points regarding ranking functions:\n",
    "\n",
    "* `ROW_NUMBER()`, `RANK()`, and other ranking functions must always be windowed and therefore cannot appear without a corresponding OVER clause.\n",
    "\n",
    "* Give consideration to how ties should be handled with ranking functions. If you need contiguous ranking, you should use `DENSE_RANK()` instead.\n",
    "\n",
    "* The `ORDER BY` predicate is mandatory for this class of functions because it influences how the results will be sequenced or ranked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark Syntax\n",
    "\n",
    "Now that we are familiar with the SQL syntax of window functions let's have a look at its PySpark equivalent.\n",
    "\n",
    "The first step to a window function is to create a window specification. \n",
    "\n",
    "Note that the `partition by` is unrelated to the partitioning scheme concept that we have covered thus far. It’s just a similar concept that describes how we will be breaking up our group. The ordering determines the ordering within a given partition. \n",
    "\n",
    "In the following example, we will reproduce our last SQL query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "windowSpec0 = Window.partitionBy()\n",
    "windowSpec1 = Window.partitionBy(\"CustomerId\")\n",
    "windowSpec2 = Window.partitionBy(\"CustomerId\", \"InvoiceDate\")\n",
    "windowSpec3 = Window.partitionBy(\"CustomerId\", \"InvoiceDate\").orderBy(desc(\"Quantity\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to use an aggregation function to learn more about each specific customer. An example might be establishing the maximum purchase quantity on each day. We indicate the window specification that defines to which frames of data this function will apply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, max, rank, dense_rank, row_number\n",
    "\n",
    "overall_dataset_Q = sum(col(\"Quantity\")).over(windowSpec0)\n",
    "total_Q = sum(col(\"Quantity\")).over(windowSpec1)\n",
    "max_Q = max(col(\"Quantity\")).over(windowSpec1)\n",
    "total_Q_by_date = sum(col(\"Quantity\")).over(windowSpec2)\n",
    "rank = rank().over(windowSpec3)\n",
    "d_rank = dense_rank().over(windowSpec3)\n",
    "row_number = row_number().over(windowSpec3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns columns that we can use in `select` statements. Now we can perform a select to view the calculated window values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------+-----------------+-------+-----+---------------+----+------+----------+\n",
      "|CustomerId|InvoiceDate|Quantity|overall_dataset_Q|total_Q|max_Q|total_Q_by_date|rank|d_rank|row_number|\n",
      "+----------+-----------+--------+-----------------+-------+-----+---------------+----+------+----------+\n",
      "|     12346| 2011-01-18|   74215|          4906888|      0|74215|              0|   1|     1|         1|\n",
      "|     12346| 2011-01-18|  -74215|          4906888|      0|74215|              0|   2|     2|         2|\n",
      "|     12347| 2011-12-07|      24|          4906888|   2458|  240|            192|   1|     1|         1|\n",
      "|     12347| 2011-12-07|      24|          4906888|   2458|  240|            192|   1|     1|         2|\n",
      "|     12347| 2011-12-07|      24|          4906888|   2458|  240|            192|   1|     1|         3|\n",
      "|     12347| 2011-12-07|      24|          4906888|   2458|  240|            192|   1|     1|         4|\n",
      "|     12347| 2011-12-07|      20|          4906888|   2458|  240|            192|   5|     2|         5|\n",
      "|     12347| 2011-12-07|      20|          4906888|   2458|  240|            192|   5|     2|         6|\n",
      "|     12347| 2011-12-07|      16|          4906888|   2458|  240|            192|   7|     3|         7|\n",
      "|     12347| 2011-12-07|      12|          4906888|   2458|  240|            192|   8|     4|         8|\n",
      "|     12347| 2011-12-07|      12|          4906888|   2458|  240|            192|   8|     4|         9|\n",
      "|     12347| 2011-12-07|      10|          4906888|   2458|  240|            192|  10|     5|        10|\n",
      "|     12347| 2011-12-07|       6|          4906888|   2458|  240|            192|  11|     6|        11|\n",
      "|     12347| 2011-10-31|      48|          4906888|   2458|  240|            676|   1|     1|         1|\n",
      "|     12347| 2011-10-31|      36|          4906888|   2458|  240|            676|   2|     2|         2|\n",
      "|     12347| 2011-10-31|      36|          4906888|   2458|  240|            676|   2|     2|         3|\n",
      "|     12347| 2011-10-31|      24|          4906888|   2458|  240|            676|   4|     3|         4|\n",
      "|     12347| 2011-10-31|      24|          4906888|   2458|  240|            676|   4|     3|         5|\n",
      "|     12347| 2011-10-31|      24|          4906888|   2458|  240|            676|   4|     3|         6|\n",
      "|     12347| 2011-10-31|      24|          4906888|   2458|  240|            676|   4|     3|         7|\n",
      "+----------+-----------+--------+-----------------+-------+-----+---------------+----+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.where(\"CustomerId IS NOT NULL\")\\\n",
    "  .select(\n",
    "    col(\"CustomerId\"),\n",
    "    col(\"InvoiceDate\"),\n",
    "    col(\"Quantity\"),\n",
    "    overall_dataset_Q.alias(\"overall_dataset_Q\"),\n",
    "    total_Q.alias(\"total_Q\"),\n",
    "    max_Q.alias(\"max_Q\"),\n",
    "    total_Q_by_date.alias(\"total_Q_by_date\"),\n",
    "    rank.alias(\"rank\"),\n",
    "    d_rank.alias(\"d_rank\"),\n",
    "    row_number.alias(\"row_number\"))\\\n",
    "  .orderBy(\"CustomerId\", desc(\"InvoiceDate\"), \"row_number\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rollups\n",
    "\n",
    "A rollup is a multidimensional aggregation that performs a variety of group-by style calculations for us.\n",
    "\n",
    "Let’s create a rollup that looks across time (with our new `InvoiceDate` column) and space (with the `Country` column) and creates a new DataFrame that includes \n",
    "- the grand total over all dates \n",
    "- the grand total for each date in the DataFrame \n",
    "- and the subtotal for each country on each date in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNoNull = df.drop()\n",
    "dfNoNull.createOrReplaceTempView(\"dfNoNull\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-------------+\n",
      "|InvoiceDate|       Country|sum(Quantity)|\n",
      "+-----------+--------------+-------------+\n",
      "|       null|          null|      5176450|\n",
      "| 2010-12-01|        France|          449|\n",
      "| 2010-12-01|     Australia|          107|\n",
      "| 2010-12-01|United Kingdom|        23949|\n",
      "| 2010-12-01|        Norway|         1852|\n",
      "| 2010-12-01|          null|        26814|\n",
      "| 2010-12-01|       Germany|          117|\n",
      "| 2010-12-01|          EIRE|          243|\n",
      "| 2010-12-01|   Netherlands|           97|\n",
      "| 2010-12-02|          EIRE|            4|\n",
      "+-----------+--------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rolledUpDF = dfNoNull.rollup(\"InvoiceDate\", \"Country\")\\\n",
    "  .agg(sum(\"Quantity\"))\\\n",
    "  .orderBy(\"InvoiceDate\")\n",
    "\n",
    "rolledUpDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now where you see the null values is where you’ll find the grand totals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+\n",
      "|InvoiceDate|Country|sum(Quantity)|\n",
      "+-----------+-------+-------------+\n",
      "| 2010-12-01|   null|        26814|\n",
      "| 2010-12-02|   null|        21023|\n",
      "| 2010-12-03|   null|        14830|\n",
      "| 2010-12-05|   null|        16395|\n",
      "| 2010-12-06|   null|        21419|\n",
      "+-----------+-------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rolledUpDF.where(\"Country IS NULL\").where(\"InvoiceDate IS NOT NULL\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A null in both rollup columns specifies the grand total across both of those columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+\n",
      "|InvoiceDate|Country|sum(Quantity)|\n",
      "+-----------+-------+-------------+\n",
      "|       null|   null|      5176450|\n",
      "+-----------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rolledUpDF.where(\"InvoiceDate IS NULL\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cube\n",
    "\n",
    "A `cube` takes the `rollup` to a level deeper. Rather than treating elements hierarchically, a cube does the same thing across all dimensions. This means that it won’t just go by date over the entire time period, but also the country. To pose this as a question again, can you make a table that includes the following?\n",
    "\n",
    "* The total across all dates and countries\n",
    "* The total for each date across all countries\n",
    "* The total for each country on each date\n",
    "* The total for each country across all dates\n",
    "\n",
    "The method call is quite similar, but instead of calling `rollup`, we call `cube`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+-------------+\n",
      "|InvoiceDate|           Country|sum(Quantity)|\n",
      "+-----------+------------------+-------------+\n",
      "|       null|           Denmark|         8188|\n",
      "|       null|            Norway|        19247|\n",
      "|       null|             Spain|        26824|\n",
      "|       null|    Czech Republic|          592|\n",
      "|       null|European Community|          497|\n",
      "+-----------+------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "cubbedDf = dfNoNull.cube(\"InvoiceDate\", \"Country\")\\\n",
    "  .agg(sum(\"Quantity\"))\\\n",
    "  .orderBy(\"InvoiceDate\")\n",
    "\n",
    "cubbedDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+\n",
      "|InvoiceDate|Country|sum(Quantity)|\n",
      "+-----------+-------+-------------+\n",
      "|       null|   null|      5176450|\n",
      "| 2010-12-01|   null|        26814|\n",
      "| 2010-12-02|   null|        21023|\n",
      "| 2010-12-03|   null|        14830|\n",
      "| 2010-12-05|   null|        16395|\n",
      "+-----------+-------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cubbedDf.where(\"Country IS NULL\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s a great way to create a quick summary table that others can use later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot\n",
    "\n",
    "Pivots make it possible for you to convert a row into a column. For example, in our current data we have a `Country` column. With a `pivot`, we can aggregate according to some function for each of those given countries and display them in an easy-to-query way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: long (nullable = true)\n",
      " |-- InvoiceDate: date (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted = df.groupBy(\"InvoiceDate\").pivot(\"Country\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This DataFrame will now have a column for every combination of country, numeric variable, and a column specifying the date. For example, for USA we have the following columns: USA_sum(Quantity), and USA_sum(UnitPrice). This represents one for each numeric column in our dataset (because we just performed an aggregation over all of them).\n",
    "\n",
    "Here’s an example query and result from this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------------+------------------------+---------------------+----------------------+---------------------+----------------------+---------------------+----------------------+--------------------+---------------------+--------------------+---------------------+-----------------------------+------------------------------+--------------------+---------------------+----------------------------+-----------------------------+---------------------+----------------------+------------------+-------------------+--------------------------------+---------------------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+---------------------+-----------------------+------------------------+---------------------+----------------------+--------------------+---------------------+-------------------+--------------------+-------------------+--------------------+---------------------+----------------------+-----------------------+------------------------+-------------------+--------------------+-------------------------+--------------------------+--------------------+---------------------+--------------------+---------------------+----------------------+-----------------------+-----------------+------------------+--------------------------+---------------------------+-----------------------+------------------------+-------------------+--------------------+--------------------+---------------------+-------------------------+--------------------------+-----------------+------------------+----------------------------------+-----------------------------------+----------------------------+-----------------------------+-------------------------+--------------------------+\n",
      "|InvoiceDate|Australia_sum(Quantity)|Australia_sum(UnitPrice)|Austria_sum(Quantity)|Austria_sum(UnitPrice)|Bahrain_sum(Quantity)|Bahrain_sum(UnitPrice)|Belgium_sum(Quantity)|Belgium_sum(UnitPrice)|Brazil_sum(Quantity)|Brazil_sum(UnitPrice)|Canada_sum(Quantity)|Canada_sum(UnitPrice)|Channel Islands_sum(Quantity)|Channel Islands_sum(UnitPrice)|Cyprus_sum(Quantity)|Cyprus_sum(UnitPrice)|Czech Republic_sum(Quantity)|Czech Republic_sum(UnitPrice)|Denmark_sum(Quantity)|Denmark_sum(UnitPrice)|EIRE_sum(Quantity)|EIRE_sum(UnitPrice)|European Community_sum(Quantity)|European Community_sum(UnitPrice)|Finland_sum(Quantity)|Finland_sum(UnitPrice)|France_sum(Quantity)|France_sum(UnitPrice)|Germany_sum(Quantity)|Germany_sum(UnitPrice)|Greece_sum(Quantity)|Greece_sum(UnitPrice)|Hong Kong_sum(Quantity)|Hong Kong_sum(UnitPrice)|Iceland_sum(Quantity)|Iceland_sum(UnitPrice)|Israel_sum(Quantity)|Israel_sum(UnitPrice)|Italy_sum(Quantity)|Italy_sum(UnitPrice)|Japan_sum(Quantity)|Japan_sum(UnitPrice)|Lebanon_sum(Quantity)|Lebanon_sum(UnitPrice)|Lithuania_sum(Quantity)|Lithuania_sum(UnitPrice)|Malta_sum(Quantity)|Malta_sum(UnitPrice)|Netherlands_sum(Quantity)|Netherlands_sum(UnitPrice)|Norway_sum(Quantity)|Norway_sum(UnitPrice)|Poland_sum(Quantity)|Poland_sum(UnitPrice)|Portugal_sum(Quantity)|Portugal_sum(UnitPrice)|RSA_sum(Quantity)|RSA_sum(UnitPrice)|Saudi Arabia_sum(Quantity)|Saudi Arabia_sum(UnitPrice)|Singapore_sum(Quantity)|Singapore_sum(UnitPrice)|Spain_sum(Quantity)|Spain_sum(UnitPrice)|Sweden_sum(Quantity)|Sweden_sum(UnitPrice)|Switzerland_sum(Quantity)|Switzerland_sum(UnitPrice)|USA_sum(Quantity)|USA_sum(UnitPrice)|United Arab Emirates_sum(Quantity)|United Arab Emirates_sum(UnitPrice)|United Kingdom_sum(Quantity)|United Kingdom_sum(UnitPrice)|Unspecified_sum(Quantity)|Unspecified_sum(UnitPrice)|\n",
      "+-----------+-----------------------+------------------------+---------------------+----------------------+---------------------+----------------------+---------------------+----------------------+--------------------+---------------------+--------------------+---------------------+-----------------------------+------------------------------+--------------------+---------------------+----------------------------+-----------------------------+---------------------+----------------------+------------------+-------------------+--------------------------------+---------------------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+---------------------+-----------------------+------------------------+---------------------+----------------------+--------------------+---------------------+-------------------+--------------------+-------------------+--------------------+---------------------+----------------------+-----------------------+------------------------+-------------------+--------------------+-------------------------+--------------------------+--------------------+---------------------+--------------------+---------------------+----------------------+-----------------------+-----------------+------------------+--------------------------+---------------------------+-----------------------+------------------------+-------------------+--------------------+--------------------+---------------------+-------------------------+--------------------------+-----------------+------------------+----------------------------------+-----------------------------------+----------------------------+-----------------------------+-------------------------+--------------------------+\n",
      "| 2011-12-06|                   null|                    null|                 null|                  null|                 null|                  null|                  396|    199.26000000000005|                null|                 null|                null|                 null|                         null|                          null|                null|                 null|                        null|                         null|                 null|                  null|              null|               null|                            null|                             null|                 null|                  null|                 787|    268.4799999999998|                  -29|                  4.15|                 236|   163.36000000000004|                   null|                    null|                 null|                  null|                null|                 null|                 45|               64.45|                -49|  19.679999999999996|                 null|                  null|                   null|                    null|               null|                null|                     null|                      null|                null|                 null|                null|                 null|                   267|      92.53000000000002|             null|              null|                      null|                       null|                   null|                    null|                182|               40.08|                null|                 null|                     null|                      null|             null|              null|                              null|                               null|                       27191|            9512.649999999947|                     null|                      null|\n",
      "| 2011-12-09|                   null|                    null|                 null|                  null|                 null|                  null|                  203|                 32.22|                null|                 null|                null|                 null|                         null|                          null|                null|                 null|                        null|                         null|                 null|                  null|              null|               null|                            null|                             null|                 null|                  null|                 105|                 44.5|                  880|    279.05999999999995|                null|                 null|                   null|                    null|                 null|                  null|                null|                 null|               null|                null|               null|                null|                 null|                  null|                   null|                    null|               null|                null|                     null|                      null|                2227|   157.00000000000003|                null|                 null|                  null|                   null|             null|              null|                      null|                       null|                   null|                    null|               null|                null|                null|                 null|                     null|                      null|             null|              null|                              null|                               null|                        9534|            8713.700000000063|                     null|                      null|\n",
      "| 2011-12-08|                   null|                    null|                  148|                 64.58|                 null|                  null|                 null|                  null|                null|                 null|                null|                 null|                           -1|                          4.25|                null|                 null|                        null|                         null|                 null|                  null|               806| 218.65999999999994|                            null|                             null|                 null|                  null|                  18|                 52.9|                  969|                192.33|                null|                 null|                   null|                    null|                 null|                  null|                null|                 null|               null|                null|               null|                null|                 null|                  null|                   null|                    null|               null|                null|                      140|                      1.79|                null|                 null|                null|                 null|                  null|                   null|             null|              null|                      null|                       null|                   null|                    null|               null|                null|                null|                 null|                     null|                      null|             -196|             13.75|                              null|                               null|                       32576|           21259.840000000513|                     null|                      null|\n",
      "| 2011-12-07|                   null|                    null|                 null|                  null|                 null|                  null|                 null|                  null|                null|                 null|                null|                 null|                         null|                          null|                null|                 null|                        null|                         null|                 null|                  null|              1998|  538.6899999999995|                            null|                             null|                   -1|                  2.08|                 561|   136.98000000000002|                 1220|                460.33|                null|                 null|                   null|                    null|                  192|                 13.54|                null|                 null|               null|                null|               null|                null|                 null|                  null|                   null|                    null|               null|                null|                     7818|                     199.6|                null|                 null|                null|                 null|                   139|                  86.18|             null|              null|                      null|                       null|                   null|                    null|                 74|  110.40000000000003|                null|                 null|                     null|                      null|             null|              null|                              null|                               null|                       27611|            7501.389999999952|                     null|                      null|\n",
      "+-----------+-----------------------+------------------------+---------------------+----------------------+---------------------+----------------------+---------------------+----------------------+--------------------+---------------------+--------------------+---------------------+-----------------------------+------------------------------+--------------------+---------------------+----------------------------+-----------------------------+---------------------+----------------------+------------------+-------------------+--------------------------------+---------------------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+---------------------+-----------------------+------------------------+---------------------+----------------------+--------------------+---------------------+-------------------+--------------------+-------------------+--------------------+---------------------+----------------------+-----------------------+------------------------+-------------------+--------------------+-------------------------+--------------------------+--------------------+---------------------+--------------------+---------------------+----------------------+-----------------------+-----------------+------------------+--------------------------+---------------------------+-----------------------+------------------------+-------------------+--------------------+--------------------+---------------------+-------------------------+--------------------------+-----------------+------------------+----------------------------------+-----------------------------------+----------------------------+-----------------------------+-------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivoted.where(\"InvoiceDate > '2011-12-05'\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------------------+\n",
      "|InvoiceDate|United Kingdom_sum(Quantity)|\n",
      "+-----------+----------------------------+\n",
      "| 2011-12-06|                       27191|\n",
      "| 2011-12-05|                       42414|\n",
      "| 2011-12-09|                        9534|\n",
      "| 2011-12-02|                       24457|\n",
      "| 2011-12-08|                       32576|\n",
      "| 2011-12-07|                       27611|\n",
      "| 2011-12-04|                       10816|\n",
      "+-----------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivoted.where(\"InvoiceDate > '2011-12-01'\").select(\"InvoiceDate\" ,\"United Kingdom_sum(Quantity)\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}